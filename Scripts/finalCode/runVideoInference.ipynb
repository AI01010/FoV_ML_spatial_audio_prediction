{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474510a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import torch\n",
    "\n",
    "from cnn_model import HeatmapFusionCNN\n",
    "from getAudioSaliency import compute_audio_saliency_heatmap_vectorized, precompute_integrals\n",
    "from getVideoLabels import filterDf, getModeTileIndex\n",
    "from getVideoSaliency import compute_video_saliency_heatmap_vectorized\n",
    "\n",
    "def normalize_heatmaps(heatmaps):\n",
    "    \"\"\"Normalize heatmap to [0, 1] range.\"\"\"\n",
    "    # returns a list of mins and maxs for each heatmap\n",
    "    h_mins = np.min(heatmaps, axis=(1, 2), keepdims=True)\n",
    "    h_maxs = np.max(heatmaps, axis=(1, 2), keepdims=True)\n",
    "\n",
    "    return (heatmaps - h_mins) / (h_maxs - h_mins)\n",
    "\n",
    "\n",
    "def getFrame(cap, output_height, output_width, frame_idx):    \n",
    "    \"\"\"\n",
    "    Read video and yield resized frames.\n",
    "    \"\"\"\n",
    "    \n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "    ret, frame = cap.read()        \n",
    "    resized_frame = cv2.resize(frame, (output_width, output_height), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    return resized_frame\n",
    "\n",
    "def tile_index_to_coords(idx, numCols):\n",
    "    \"\"\"Convert linear index to tile coordinates\"\"\"\n",
    "    y = idx // numCols\n",
    "    x = idx % numCols\n",
    "    return x, y\n",
    "\n",
    "def tile_distance(pred_idx, true_idx, numCols):\n",
    "    \"\"\"Calculate tile distance\"\"\"\n",
    "    px, py = tile_index_to_coords(pred_idx, numCols)\n",
    "    print(f\"{px}, {py}\")\n",
    "    tx, ty = tile_index_to_coords(true_idx, numCols)\n",
    "    print(f\"{tx}, {ty}\")\n",
    "    distance =  (min((px - tx), numCols - (px - tx)) ** 2 + (py - ty)**2) ** 0.5\n",
    "    return distance\n",
    "\n",
    "def printAndWriteLine(printedLine, file):\n",
    "    file.writelines(printedLine + \"\\n\")\n",
    "    print(printedLine)\n",
    "\n",
    "\n",
    "def process_360_video(video_name, video_path, audio_path, output_path, model_path,\n",
    "                      csv_path, erp_height=1920, erp_width=3840, \n",
    "                      sample_every_n_frames=5, numHeatmaps=7,\n",
    "                      cols = 16, rows = 9, device = \"cpu\"):\n",
    "    \"\"\"\n",
    "    Main pipeline to process a 360 video and extract audio saliency heatmaps.\n",
    "    \n",
    "    Parameters:\n",
    "        video_path: path to ERP format 360 video\n",
    "        audio_path: path to first-order ambisonic audio file\n",
    "        output_path: where to save the output .npy file\n",
    "        erp_height: height of ERP format (pixels)\n",
    "        erp_width: width of ERP format (pixels)\n",
    "        sample_every_n_frames: sample every N frames\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load audio\n",
    "    print(\"Loading ambisonic audio...\")\n",
    "    audio_data, audio_samplerate = sf.read(audio_path)\n",
    "    \n",
    "    # Check for 4 channels\n",
    "    if len(audio_data.shape) == 1:\n",
    "        raise ValueError(f\"Audio is mono. Expected 4-channel first-order ambisonics.\")\n",
    "    elif audio_data.shape[1] != 4:\n",
    "        raise ValueError(f\"Audio has {audio_data.shape[1]} channels. Expected 4-channel first-order ambisonics (W, X, Y, Z).\")\n",
    "    \n",
    "    # Split into channels\n",
    "    W = audio_data[:, 0]\n",
    "    X = audio_data[:, 1]\n",
    "    Y = audio_data[:, 2]\n",
    "    Z = audio_data[:, 3]\n",
    "    \n",
    "    print(f\"Audio shape: {audio_data.shape}\")\n",
    "    print(f\"Audio sample rate: {audio_samplerate} Hz\")\n",
    "    print(\"Successfully loaded 4-channel first-order ambisonics audio\")\n",
    "    \n",
    "    # Open video to get metadata\n",
    "    print(\"Opening video...\")\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(f\"Could not open video: {video_path}\")\n",
    "    \n",
    "    video_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    video_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    video_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    print(f\"Video FPS: {video_fps}\")\n",
    "    print(f\"Total frames: {total_frames}\")\n",
    "    print(f\"Video dimensions: {video_width}x{video_height}\")\n",
    "    \n",
    "    # Check if resizing is needed\n",
    "    need_resize = video_width != erp_width or video_height != erp_height\n",
    "    if need_resize:\n",
    "        print(f\"Video will be resized from {video_width}x{video_height} to {erp_width}x{erp_height}\")\n",
    "    \n",
    "    # Precompute integrals for coarse tiles (20x20 degrees)\n",
    "    tile_cache = precompute_integrals(tile_size_deg=20)\n",
    "    \n",
    "    # Calculate number of sampled frames\n",
    "    # num_sampled_frames = (total_frames - math.ceil(sample_every_n_frames / 2)) // sample_every_n_frames\n",
    "    num_sampled_frames = 1000\n",
    "\n",
    "    # num_sampled_frames = 3\n",
    "\n",
    "    (labelDf, participants) = filterDf(csv_path, video_name, video_name)\n",
    "        \n",
    "    print(f\"Processing {num_sampled_frames} frames...\")\n",
    "\n",
    "    print(f\"Loading model...\")\n",
    "    # Load the model state\n",
    "    model = HeatmapFusionCNN()  # Create a new model instance first\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)  # Move to appropriate device\n",
    "    print(f\"Model loaded!\")\n",
    "\n",
    "    numCorrect = 0\n",
    "    numTotal = 0\n",
    "\n",
    "    predictedLabels = []\n",
    "    trueLabels = []\n",
    "    totalDistance = 0\n",
    "\n",
    "    with open(output_path, 'w') as file:\n",
    "\n",
    "            \n",
    "        # Use frame generator (resizes all frames upfront in the stream). Also, only retrieves them one at a time, instead of keeping it all in memory\n",
    "        for sampled_frame_idx in range(num_sampled_frames):\n",
    "            frame_idx = sample_every_n_frames * (sampled_frame_idx + 1)\n",
    "\n",
    "            prevFrame = getFrame(cap, erp_height, erp_width, frame_idx - 1)\n",
    "            frame = getFrame(cap, erp_height, erp_width, frame_idx)\n",
    "            \n",
    "            printedLine = f\"Processing frame {frame_idx}/{total_frames} (sample {sampled_frame_idx}/{num_sampled_frames})\"\n",
    "            printAndWriteLine(printedLine, file)\n",
    "            \n",
    "            # Compute audio saliency heatmap\n",
    "            saliency_heatmaps = np.concatenate([compute_audio_saliency_heatmap_vectorized(W, X, Y, Z, audio_samplerate,\n",
    "                                                                            frame_idx, video_fps,\n",
    "                                                                            erp_height, erp_width,\n",
    "                                                                            tile_cache, sample_every_n_frames,\n",
    "                                                                            numHeatmaps-2, tile_size_deg=20),\n",
    "                                                                            compute_video_saliency_heatmap_vectorized(prevFrame, frame, frame_idx, video_fps,\n",
    "                                                                                                                erp_height, erp_width,\n",
    "                                                                                                                tile_cache, sample_every_n_frames,\n",
    "                                                                                                                numHeatmaps-7, tile_size_deg=20)], axis=0\n",
    "                                                                            )\n",
    "            \n",
    "            # Normalize heatmap\n",
    "            saliency_heatmaps = normalize_heatmaps(saliency_heatmaps)\n",
    "\n",
    "            heatmaps = torch.from_numpy(saliency_heatmaps).float().to(device)\n",
    "\n",
    "            # Run inference\n",
    "            with torch.no_grad():\n",
    "                outputs = model(heatmaps.unsqueeze(0))\n",
    "                predicted_tile = outputs[0].argmax(dim=0).item()\n",
    "\n",
    "            targetTime = frame_idx / video_fps\n",
    "\n",
    "            actual_tile = getModeTileIndex(targetTime, labelDf, participants, rows, cols)\n",
    "\n",
    "            printedLine = f\"Predicted tile was {predicted_tile}, actual tile was {actual_tile}!\"\n",
    "            printAndWriteLine(printedLine, file)\n",
    "\n",
    "            predictedLabels.append(predicted_tile)\n",
    "            trueLabels.append(actual_tile)\n",
    "\n",
    "            if(predicted_tile == actual_tile):\n",
    "                numCorrect += 1\n",
    "\n",
    "            numTotal += 1\n",
    "\n",
    "            printedLine = f\"Num correct thus far is {numCorrect}, num total thus far is {numTotal}\"\n",
    "            printAndWriteLine(printedLine, file)\n",
    "\n",
    "            distance = tile_distance(predicted_tile, actual_tile, cols)\n",
    "\n",
    "            printedLine = f\"Euclidean distance from true was {distance}\"\n",
    "            printAndWriteLine(printedLine, file)\n",
    "\n",
    "            totalDistance += distance\n",
    "\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "            del heatmaps, outputs  # After you've extracted predicted_tile\n",
    "        \n",
    "        classes_present = np.unique(np.concatenate([predictedLabels, trueLabels]))\n",
    "        \n",
    "        cm = confusion_matrix(trueLabels, predictedLabels, labels=classes_present)\n",
    "\n",
    "        printedLine = str(cm)\n",
    "        printAndWriteLine(cm, file)\n",
    "\n",
    "        printedLine = f\"Avg distance was: {float(totalDistance) / numTotal:.2f}\"\n",
    "        printAndWriteLine(printedLine, file)\n",
    "\n",
    "        printedLine = f\"Accuracy was: {float(numCorrect) / numTotal:.2f}\"\n",
    "        printAndWriteLine(printedLine, file)\n",
    "        \n",
    "        cap.release()\n",
    "\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    os.chdir(\"./../..\")\n",
    "    \n",
    "    # Configuration - modify as needed\n",
    "    ERP_WIDTH = 1920  # width\n",
    "    ERP_HEIGHT = 960  # height\n",
    "    SAMPLE_RATE = 5  # sample every 5 frames\n",
    "    FILE_NAME = \"0004\"\n",
    "     \n",
    "    VIDEO_PATH = f\"Data/Pre-Processed-Data/{FILE_NAME}/{FILE_NAME}_mono_60fps.mp4\"  # ERP format 360 video\n",
    "    AUDIO_PATH = f\"Data/Pre-Processed-Data/{FILE_NAME}/{FILE_NAME}.wav\"\n",
    "    INPUT_CSV_PATH = f\"Data/Pre-Processed-Data/head_data/head_video_{FILE_NAME}.csv\"\n",
    "    OUTPUT_PATH = f\"FinalTestingResults/{FILE_NAME}_Results.txt\"\n",
    "    MODEL_PATH = f\"Scripts/FinalCode/cnn_model.pth\"\n",
    "    NUM_HEATMAPS = 9\n",
    "    TILE_COLS = 16\n",
    "    TILE_ROWS = 9\n",
    "\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Run the pipeline\n",
    "    process_360_video(FILE_NAME, VIDEO_PATH, AUDIO_PATH, OUTPUT_PATH, MODEL_PATH, INPUT_CSV_PATH,\n",
    "                                      erp_height=ERP_HEIGHT, erp_width=ERP_WIDTH,\n",
    "                                      sample_every_n_frames=SAMPLE_RATE, numHeatmaps=NUM_HEATMAPS,\n",
    "                                      cols=TILE_COLS, rows=TILE_ROWS, device = DEVICE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
