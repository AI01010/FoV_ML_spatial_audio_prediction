{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474510a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo path was: c:\\Users\\mahd\\Documents\\FOV Prediction\\Scripts\\finalCode\\../../U-2-Net-Repo\n",
      "Using device: cpu\n",
      "U2 Net Model ready!\n",
      "Successfully imported core RAFT modules from your fork.\n",
      "Loading RAFT cpu model...\n",
      "RAFT model loaded on cpu successfully.\n",
      "Loading ambisonic audio...\n",
      "Audio shape: (2880000, 4)\n",
      "Audio sample rate: 48000 Hz\n",
      "Successfully loaded 4-channel first-order ambisonics audio\n",
      "Opening video...\n",
      "Video FPS: 60.0\n",
      "Total frames: 3598\n",
      "Video dimensions: 3840x1920\n",
      "Video will be resized from 3840x1920 to 1920x960\n",
      "Precomputing integrals for 20° tiles...\n",
      "Integral precomputation complete!\n",
      "Reading CSV file: Data/Pre-Processed-Data/head_data/head_video_0004.csv\n",
      "Processing 2 frames...\n",
      "Loading model...\n",
      "Model loaded!\n",
      "Processing frame 5/3598 (sample 0/2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mahd\\Documents\\FOV Prediction\\Scripts\\finalCode\\../../U-2-Net-Repo\\model\\u2net.py:23: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n",
      "  src = F.upsample(src,size=tar.shape[2:],mode='bilinear')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trying scale 1.0...\n",
      "Padded tensor shapes: t1=torch.Size([1, 3, 960, 1920]), t2=torch.Size([1, 3, 960, 1920])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mahd\\Documents\\FOV Prediction\\Scripts\\finalCode\\../../Hani-Raft-Repo/core\\raft.py:99: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=self.args.mixed_precision):\n",
      "c:\\Users\\mahd\\Documents\\FOV Prediction\\Scripts\\finalCode\\../../Hani-Raft-Repo/core\\raft.py:110: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=self.args.mixed_precision):\n",
      "c:\\Users\\mahd\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\functional.py:505: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:4319.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "c:\\Users\\mahd\\Documents\\FOV Prediction\\Scripts\\finalCode\\../../Hani-Raft-Repo/core\\raft.py:127: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=self.args.mixed_precision):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Flow computed successfully at scale 1.0\n",
      "before fusion: 0.0 1.0\n",
      "After fusion: -0.8454725742340088 1.0559604167938232\n",
      "After conv1: -0.7022640109062195 0.7890627980232239\n",
      "After conv2: -4.248074054718018 4.467042446136475\n",
      "After conv3: -2.8335306644439697 3.3739211559295654\n",
      "After pool: 0.0 1.8778979778289795\n",
      "After fc1: -1.508062720298767 5.066052436828613\n",
      "After fc2 (output): -4.561481952667236 4.552745342254639\n",
      "Predicted tile was 75, actual tile was 65!\n",
      "Num correct thus far is 0, num total thus far is 1\n",
      "Euclidean distance from true was 6.0\n",
      "Processing frame 10/3598 (sample 1/2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mahd\\Documents\\FOV Prediction\\Scripts\\finalCode\\../../U-2-Net-Repo\\model\\u2net.py:23: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n",
      "  src = F.upsample(src,size=tar.shape[2:],mode='bilinear')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trying scale 1.0...\n",
      "Padded tensor shapes: t1=torch.Size([1, 3, 960, 1920]), t2=torch.Size([1, 3, 960, 1920])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mahd\\Documents\\FOV Prediction\\Scripts\\finalCode\\../../Hani-Raft-Repo/core\\raft.py:99: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=self.args.mixed_precision):\n",
      "c:\\Users\\mahd\\Documents\\FOV Prediction\\Scripts\\finalCode\\../../Hani-Raft-Repo/core\\raft.py:110: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=self.args.mixed_precision):\n",
      "c:\\Users\\mahd\\Documents\\FOV Prediction\\Scripts\\finalCode\\../../Hani-Raft-Repo/core\\raft.py:127: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=self.args.mixed_precision):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Flow computed successfully at scale 1.0\n",
      "before fusion: 0.0 1.0\n",
      "After fusion: -0.8078544735908508 0.9146549701690674\n",
      "After conv1: -0.6568062901496887 0.6594387888908386\n",
      "After conv2: -3.9136910438537598 2.9638423919677734\n",
      "After conv3: -2.5999231338500977 3.427844285964966\n",
      "After pool: 0.0 2.238872766494751\n",
      "After fc1: -1.2006548643112183 4.9988694190979\n",
      "After fc2 (output): -4.796249866485596 8.050904273986816\n",
      "Predicted tile was 77, actual tile was 66!\n",
      "Num correct thus far is 0, num total thus far is 2\n",
      "Euclidean distance from true was 5.0\n",
      "Confusion Matrix (Predicted vs True):\n",
      "Classes: [65 66 75 77]\n",
      "[[0 0 1 0]\n",
      " [0 0 0 1]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n",
      "Avg distance was: 5.50\n",
      "Accuracy was: 0.00\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import torch\n",
    "\n",
    "from cnn_model import HeatmapFusionCNN\n",
    "from getAudioSaliency import compute_audio_saliency_heatmap_vectorized, precompute_integrals\n",
    "from getVideoLabels import filterDf, getModeTileIndex\n",
    "from getVideoSaliency import compute_video_saliency_heatmap_vectorized\n",
    "\n",
    "def normalize_heatmaps(heatmaps):\n",
    "    \"\"\"Normalize heatmap to [0, 1] range.\"\"\"\n",
    "    # returns a list of mins and maxs for each heatmap\n",
    "    h_mins = np.min(heatmaps, axis=(1, 2), keepdims=True)\n",
    "    h_maxs = np.max(heatmaps, axis=(1, 2), keepdims=True)\n",
    "\n",
    "    return (heatmaps - h_mins) / (h_maxs - h_mins)\n",
    "\n",
    "\n",
    "def getFrame(cap, output_height, output_width, frame_idx):    \n",
    "    \"\"\"\n",
    "    Read video and yield resized frames.\n",
    "    \"\"\"\n",
    "    \n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "    ret, frame = cap.read()        \n",
    "    resized_frame = cv2.resize(frame, (output_width, output_height), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    return resized_frame\n",
    "\n",
    "def tile_index_to_coords(idx, numCols):\n",
    "    \"\"\"Convert linear index to tile coordinates\"\"\"\n",
    "    y = idx // numCols\n",
    "    x = idx % numCols\n",
    "    return x, y\n",
    "\n",
    "def tile_distance(pred_idx, true_idx, numCols):\n",
    "    \"\"\"Calculate tile distance\"\"\"\n",
    "    px, py = tile_index_to_coords(pred_idx, numCols)\n",
    "    tx, ty = tile_index_to_coords(true_idx, numCols)\n",
    "    \n",
    "    # Wrap horizontally\n",
    "    dx = abs(px - tx)\n",
    "    dx = min(dx, numCols - dx)\n",
    "    \n",
    "    # Don't wrap vertically\n",
    "    dy = abs(py - ty)\n",
    "    \n",
    "    distance = (dx**2 + dy**2) ** 0.5\n",
    "    return distance\n",
    "\n",
    "def printAndWriteLine(printedLine, file):\n",
    "    file.write(printedLine + \"\\n\")\n",
    "    print(printedLine)\n",
    "\n",
    "\n",
    "def process_360_video(video_name, video_path, audio_path, output_path, model_path,\n",
    "                      csv_path, erp_height=1920, erp_width=3840, \n",
    "                      sample_every_n_frames=5, numHeatmaps=7,\n",
    "                      cols = 16, rows = 9, device = \"cpu\"):\n",
    "    \"\"\"\n",
    "    Main pipeline to process a 360 video and extract audio saliency heatmaps.\n",
    "    \n",
    "    Parameters:\n",
    "        video_path: path to ERP format 360 video\n",
    "        audio_path: path to first-order ambisonic audio file\n",
    "        output_path: where to save the output .npy file\n",
    "        erp_height: height of ERP format (pixels)\n",
    "        erp_width: width of ERP format (pixels)\n",
    "        sample_every_n_frames: sample every N frames\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load audio\n",
    "    print(\"Loading ambisonic audio...\")\n",
    "    audio_data, audio_samplerate = sf.read(audio_path)\n",
    "    \n",
    "    # Check for 4 channels\n",
    "    if len(audio_data.shape) == 1:\n",
    "        raise ValueError(f\"Audio is mono. Expected 4-channel first-order ambisonics.\")\n",
    "    elif audio_data.shape[1] != 4:\n",
    "        raise ValueError(f\"Audio has {audio_data.shape[1]} channels. Expected 4-channel first-order ambisonics (W, X, Y, Z).\")\n",
    "    \n",
    "    # Split into channels\n",
    "    W = audio_data[:, 0]\n",
    "    X = audio_data[:, 1]\n",
    "    Y = audio_data[:, 2]\n",
    "    Z = audio_data[:, 3]\n",
    "    \n",
    "    print(f\"Audio shape: {audio_data.shape}\")\n",
    "    print(f\"Audio sample rate: {audio_samplerate} Hz\")\n",
    "    print(\"Successfully loaded 4-channel first-order ambisonics audio\")\n",
    "    \n",
    "    # Open video to get metadata\n",
    "    print(\"Opening video...\")\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(f\"Could not open video: {video_path}\")\n",
    "    \n",
    "    video_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    video_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    video_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    print(f\"Video FPS: {video_fps}\")\n",
    "    print(f\"Total frames: {total_frames}\")\n",
    "    print(f\"Video dimensions: {video_width}x{video_height}\")\n",
    "    \n",
    "    # Check if resizing is needed\n",
    "    need_resize = video_width != erp_width or video_height != erp_height\n",
    "    if need_resize:\n",
    "        print(f\"Video will be resized from {video_width}x{video_height} to {erp_width}x{erp_height}\")\n",
    "    \n",
    "    # Precompute integrals for coarse tiles (20x20 degrees)\n",
    "    tile_cache = precompute_integrals(tile_size_deg=20)\n",
    "    \n",
    "    # Calculate number of sampled frames\n",
    "    # num_sampled_frames = (total_frames - math.ceil(sample_every_n_frames / 2)) // sample_every_n_frames\n",
    "    num_sampled_frames = 150    \n",
    "\n",
    "    # num_sampled_frames = 3\n",
    "\n",
    "    (labelDf, participants) = filterDf(csv_path, video_name, video_name)\n",
    "        \n",
    "    print(f\"Processing {num_sampled_frames} frames...\")\n",
    "\n",
    "    print(f\"Loading model...\")\n",
    "    # Load the model state\n",
    "    model = HeatmapFusionCNN()  # Create a new model instance first\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)  # Move to appropriate device\n",
    "    print(f\"Model loaded!\")\n",
    "\n",
    "    numCorrect = 0\n",
    "    numTotal = 0\n",
    "\n",
    "    predictedLabels = []\n",
    "    trueLabels = []\n",
    "    totalDistance = 0\n",
    "\n",
    "    with open(output_path, 'w') as file:\n",
    "\n",
    "            \n",
    "        # Use frame generator (resizes all frames upfront in the stream). Also, only retrieves them one at a time, instead of keeping it all in memory\n",
    "        for sampled_frame_idx in range(num_sampled_frames):\n",
    "            frame_idx = sample_every_n_frames * (sampled_frame_idx + 1)\n",
    "\n",
    "            prevFrame = getFrame(cap, erp_height, erp_width, frame_idx - 1)\n",
    "            frame = getFrame(cap, erp_height, erp_width, frame_idx)\n",
    "            \n",
    "            printedLine = f\"Processing frame {frame_idx}/{total_frames} (sample {sampled_frame_idx}/{num_sampled_frames})\"\n",
    "            printAndWriteLine(printedLine, file)\n",
    "            \n",
    "            # Compute audio saliency heatmap\n",
    "            saliency_heatmaps = np.concatenate([compute_audio_saliency_heatmap_vectorized(W, X, Y, Z, audio_samplerate,\n",
    "                                                                            frame_idx, video_fps,\n",
    "                                                                            erp_height, erp_width,\n",
    "                                                                            tile_cache, sample_every_n_frames,\n",
    "                                                                            numHeatmaps-2, tile_size_deg=20),\n",
    "                                                                            compute_video_saliency_heatmap_vectorized(prevFrame, frame, frame_idx, video_fps,\n",
    "                                                                                                                erp_height, erp_width,\n",
    "                                                                                                                tile_cache, sample_every_n_frames,\n",
    "                                                                                                                numHeatmaps-7, tile_size_deg=20)], axis=0\n",
    "                                                                            )\n",
    "            \n",
    "            # Normalize heatmap\n",
    "            saliency_heatmaps = normalize_heatmaps(saliency_heatmaps)\n",
    "\n",
    "            heatmaps = torch.from_numpy(saliency_heatmaps).float().to(device)\n",
    "\n",
    "            # Run inference\n",
    "            with torch.no_grad():\n",
    "                outputs = model(heatmaps.unsqueeze(0))\n",
    "                predicted_tile = outputs[0].argmax(dim=0).item()\n",
    "\n",
    "            targetTime = frame_idx / video_fps\n",
    "\n",
    "            actual_tile = getModeTileIndex(targetTime, labelDf, participants, rows, cols)\n",
    "\n",
    "            printedLine = f\"Predicted tile was {predicted_tile}, actual tile was {actual_tile}!\"\n",
    "            printAndWriteLine(printedLine, file)\n",
    "\n",
    "            predictedLabels.append(predicted_tile)\n",
    "            trueLabels.append(actual_tile)\n",
    "\n",
    "            if(predicted_tile == actual_tile):\n",
    "                numCorrect += 1\n",
    "\n",
    "            numTotal += 1\n",
    "\n",
    "            printedLine = f\"Num correct thus far is {numCorrect}, num total thus far is {numTotal}\"\n",
    "            printAndWriteLine(printedLine, file)\n",
    "\n",
    "            distance = tile_distance(predicted_tile, actual_tile, cols)\n",
    "\n",
    "            printedLine = f\"Euclidean distance from true was {distance}\"\n",
    "            printAndWriteLine(printedLine, file)\n",
    "\n",
    "            totalDistance += distance\n",
    "\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "            del heatmaps, outputs  # After you've extracted predicted_tile\n",
    "        \n",
    "        classes_present = np.unique(np.concatenate([predictedLabels, trueLabels]))\n",
    "        \n",
    "        cm = confusion_matrix(trueLabels, predictedLabels, labels=classes_present)\n",
    "\n",
    "        # Format with labels\n",
    "        cm_str = f\"Confusion Matrix (Predicted vs True):\\n\"\n",
    "        cm_str += f\"Classes: {classes_present}\\n\"\n",
    "        cm_str += str(cm)\n",
    "\n",
    "        printAndWriteLine(cm_str, file)\n",
    "\n",
    "        printedLine = f\"Avg distance was: {float(totalDistance) / numTotal:.2f}\"\n",
    "        printAndWriteLine(printedLine, file)\n",
    "\n",
    "        printedLine = f\"Accuracy was: {float(numCorrect) / numTotal:.2f}\"\n",
    "        printAndWriteLine(printedLine, file)\n",
    "        \n",
    "        cap.release()\n",
    "\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    os.chdir(\"./../..\")\n",
    "    \n",
    "    # Configuration - modify as needed\n",
    "    ERP_WIDTH = 1920  # width\n",
    "    ERP_HEIGHT = 960  # height\n",
    "    SAMPLE_RATE = 5  # sample every 5 frames\n",
    "    FILE_NAME = \"0004\"\n",
    "     \n",
    "    VIDEO_PATH = f\"Data/Pre-Processed-Data/{FILE_NAME}/{FILE_NAME}_mono_60fps.mp4\"  # ERP format 360 video\n",
    "    AUDIO_PATH = f\"Data/Pre-Processed-Data/{FILE_NAME}/{FILE_NAME}.wav\"\n",
    "    INPUT_CSV_PATH = f\"Data/Pre-Processed-Data/head_data/head_video_{FILE_NAME}.csv\"\n",
    "    OUTPUT_PATH = f\"FinalTestingResults/{FILE_NAME}_Results.txt\"\n",
    "    MODEL_PATH = f\"cnn_model.pth\"\n",
    "    NUM_HEATMAPS = 9\n",
    "    TILE_COLS = 16\n",
    "    TILE_ROWS = 9\n",
    "\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Run the pipeline\n",
    "    process_360_video(FILE_NAME, VIDEO_PATH, AUDIO_PATH, OUTPUT_PATH, MODEL_PATH, INPUT_CSV_PATH,\n",
    "                                      erp_height=ERP_HEIGHT, erp_width=ERP_WIDTH,\n",
    "                                      sample_every_n_frames=SAMPLE_RATE, numHeatmaps=NUM_HEATMAPS,\n",
    "                                      cols=TILE_COLS, rows=TILE_ROWS, device = DEVICE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
