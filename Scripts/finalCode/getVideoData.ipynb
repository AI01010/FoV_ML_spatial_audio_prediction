{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a3f0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "from scipy.integrate import dblquad\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def compute_audio_at_direction(W: np.ndarray, X: np.ndarray, \n",
    "                               Y: np.ndarray, Z: np.ndarray,\n",
    "                               top_left: tuple, bottom_right: tuple,\n",
    "                               center_time: float, sampleRate: int,\n",
    "                               window_sec: float = 0.1) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute audio waveform for a tile and around a specific time.\n",
    "    \n",
    "    Parameters:\n",
    "        center_time: time in seconds to center the window\n",
    "        window_sec: half-length of window (seconds) to extract\n",
    "    \"\"\"\n",
    "    top_left_lat, top_left_lon = top_left\n",
    "    bottom_right_lat, bottom_right_lon = bottom_right\n",
    "    \n",
    "    # Convert bounds to radians\n",
    "    lat_min_rad = np.radians(bottom_right_lat)\n",
    "    lat_max_rad = np.radians(top_left_lat)\n",
    "    lon_min_rad = np.radians(top_left_lon)\n",
    "    lon_max_rad = np.radians(bottom_right_lon)\n",
    "    \n",
    "    # Compute patch area (steradians)\n",
    "    area = (lon_max_rad - lon_min_rad) * (np.sin(lat_max_rad) - np.sin(lat_min_rad))\n",
    "    \n",
    "    # Integrate Y_W over the region\n",
    "    integral_Y_W, _ = dblquad(lambda lon, lat: np.cos(lat), lat_min_rad, lat_max_rad,\n",
    "                              lambda _: lon_min_rad, lambda _: lon_max_rad)\n",
    "    integral_Y_X, _ = dblquad(lambda lon, lat: np.cos(lat)*np.cos(lon)*np.cos(lat), lat_min_rad, lat_max_rad,\n",
    "                              lambda _: lon_min_rad, lambda _: lon_max_rad)\n",
    "    integral_Y_Y, _ = dblquad(lambda lon, lat: np.cos(lat)*np.sin(lon)*np.cos(lat), lat_min_rad, lat_max_rad,\n",
    "                              lambda _: lon_min_rad, lambda _: lon_max_rad)\n",
    "    integral_Y_Z, _ = dblquad(lambda lon, lat: np.sin(lat)*np.cos(lat), lat_min_rad, lat_max_rad,\n",
    "                              lambda _: lon_min_rad, lambda _: lon_max_rad)\n",
    "    \n",
    "    # Convert time to sample indices\n",
    "    center_sample = int(center_time * sampleRate)\n",
    "    half_window_samples = int(window_sec * sampleRate)\n",
    "    start = max(center_sample - half_window_samples, 0)\n",
    "    end = min(center_sample + half_window_samples, len(W))\n",
    "    \n",
    "    # Extract the waveform slice\n",
    "    W_slice = W[start:end]\n",
    "    X_slice = X[start:end]\n",
    "    Y_slice = Y[start:end]\n",
    "    Z_slice = Z[start:end]\n",
    "    \n",
    "    # Reconstruct waveform for this tile\n",
    "    wave = (integral_Y_W * W_slice + integral_Y_X * X_slice +\n",
    "            integral_Y_Y * Y_slice + integral_Y_Z * Z_slice) / area\n",
    "    \n",
    "    return wave\n",
    "\n",
    "def computeHNR(frame):\n",
    "    \"\"\"\n",
    "    Compute HNR for a single frame using autocorrelation.\n",
    "    HNR = 10 * log10(energy_harmonic / energy_noise)\n",
    "    \"\"\"\n",
    "    frame = frame - np.mean(frame)  # remove DC\n",
    "    if np.all(frame == 0):\n",
    "        return 0.0\n",
    "\n",
    "    # FFT-based autocorrelation\n",
    "    autocorr = np.fft.irfft(np.fft.rfft(frame) * np.conj(np.fft.rfft(frame)))\n",
    "    autocorr = autocorr / np.max(np.abs(autocorr))  # normalize\n",
    "\n",
    "    # Harmonic energy = max autocorr (excluding lag 0)\n",
    "    harmonic_energy = np.max(autocorr[1:])\n",
    "    # Noise energy = lag 0 minus harmonic energy\n",
    "    noise_energy = autocorr[0] - harmonic_energy\n",
    "    if noise_energy <= 0:\n",
    "        return 40.0  # cap to a reasonable max\n",
    "    return 10 * np.log10(harmonic_energy / noise_energy)\n",
    "\n",
    "def processWave(wave, sampleRate):\n",
    "    windowSize = 2048\n",
    "    hopSize = 100\n",
    "    \n",
    "    # converts this to a Short-Time Fourier Transform. Tells you how much eergy has at each frequency over time.\n",
    "    # does this by going through windows. Length of each window defined by n_fft. Then, shifts window to right by length\n",
    "    # hop length. at each window, computes how much of each frequency is present.\n",
    "    # final value is 2D array of rows being each frequency, columns being time (which is now the windows), so value being amplitude/energy for that time and frequency\n",
    "    stftWave = np.abs(librosa.stft(wave, n_fft=windowSize, hop_length=hopSize))\n",
    "    # when we get the mel, that just converts all the frequencies to 128 possible onces, which are moreso frequencies humans can hear. So compressing\n",
    "    # the frequencies from a large number of frequencies to a smaller number, in this case n_mels amount\n",
    "    mel = librosa.feature.melspectrogram(S = stftWave, sr= sampleRate, n_mels = 128)\n",
    "    # converts from power scaling of audio to decibel scaling, cause humans perceive in moreso logarithm of audio (so higher sounds kinda taper off to us)\n",
    "    logMel = librosa.power_to_db(mel, ref=np.max)\n",
    "\n",
    "    # gets overall frame energy, including amplitude\n",
    "    volumeNorm = np.mean((logMel + 80), axis=0)\n",
    "    # gets the contrast in energy between frequencies within a specific frequency band, so where some frequencies bands may have parts of high energy frequencies, while other parts are low energy\n",
    "    contrast = librosa.feature.spectral_contrast(S = stftWave, sr=sampleRate)\n",
    "    # combines the difference frequency bands to get a average contrast for that time frame\n",
    "    contrast = np.mean(contrast, axis=0)\n",
    "    # basically gets how much the sound chagnes over time. Does this by getting differnece over time fimes with np.diff, squaring that value, and getting its sum\n",
    "    temporal_novelty = np.sum(np.diff(logMel, axis=1) ** 2, axis=0)\n",
    "    # do this to add an extra value cause rn, the length is T - 1, since you're getting difference between frames. So add 1 to get it to T length\n",
    "    temporal_novelty = np.insert(temporal_novelty, 0, 0)\n",
    "\n",
    "    # this gets how noise like a sound is, whether it's tonal or liek white noise. A tonal sound is one that just\n",
    "    # stands out, like through sharp peaks.\n",
    "    spectral_flatness = librosa.feature.spectral_flatness(y=wave, n_fft=windowSize, hop_length= hopSize)\n",
    "    # gets the average frequency weighted by amplitude, how \"bright\" the sound is, sees if the audio tends to have more high frequency or low frequency sounds\n",
    "    centroid = librosa.feature.spectral_centroid(S=stftWave, sr=sampleRate)\n",
    "    # indicates the range of frequencies present, so if the frequencies are more concentrated or spread out\n",
    "    bandwidth = librosa.feature.spectral_bandwidth(S=stftWave, sr=sampleRate)\n",
    "\n",
    "\n",
    "\n",
    "    # Compute HNR per frame. HNR is how harmonic the sound is, if its harmonic, with a pattern, or more noisy.\n",
    "    # different from spectral flatness in that it measures if its harmonic, as opposed to tonal. Basically if there's like\n",
    "    # a repeating pattern that identifies the town\n",
    "    hnr_values = []\n",
    "    num_frames = stftWave.shape[1]\n",
    "    for i in range(num_frames):\n",
    "        frame = wave[i*hopSize : i*hopSize + windowSize]\n",
    "        if len(frame) < 2:\n",
    "            continue\n",
    "        hnr_values.append(computeHNR(frame))\n",
    "    hnr_values = np.array(hnr_values)\n",
    "    \n",
    "    return np.array([np.mean(volumeNorm), np.mean(contrast), np.mean(temporal_novelty), np.mean(hnr_values), np.mean(spectral_flatness), np.mean(centroid), np.mean(bandwidth)])\n",
    "\n",
    "def precompute_integrals(tile_size_deg=20):\n",
    "    \"\"\"\n",
    "    Precompute integrals for coarse tiles (done once, cached for all frames).\n",
    "    \n",
    "    Parameters:\n",
    "        tile_size_deg: size of each tile in degrees (default 20x20)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping (lat, lon) tile coordinates to integral values and area\n",
    "    \"\"\"\n",
    "    print(f\"Precomputing integrals for {tile_size_deg}Â° tiles...\")\n",
    "    \n",
    "    tile_cache = {}\n",
    "    \n",
    "    # Generate all tile coordinates\n",
    "    latitudes = list(range(90, -90, -tile_size_deg))\n",
    "    longitudes = list(range(-180, 180, tile_size_deg))\n",
    "        \n",
    "    for topLeftLat in latitudes:\n",
    "        for topLeftLon in longitudes:            \n",
    "            bottom_right_lat, bottom_right_lon = (topLeftLat - tile_size_deg, topLeftLon + tile_size_deg)\n",
    "            \n",
    "            # Convert bounds to radians\n",
    "            lat_min_rad = np.radians(bottom_right_lat)\n",
    "            lat_max_rad = np.radians(topLeftLat)\n",
    "            lon_min_rad = np.radians(topLeftLon)\n",
    "            lon_max_rad = np.radians(bottom_right_lon)\n",
    "            \n",
    "            # Compute patch area\n",
    "            area = (lon_max_rad - lon_min_rad) * (np.sin(lat_max_rad) - np.sin(lat_min_rad))\n",
    "            \n",
    "            # Compute integrals\n",
    "            integral_Y_W, _ = dblquad(\n",
    "                lambda lon, lat: np.cos(lat), lat_min_rad, lat_max_rad,\n",
    "                lambda _: lon_min_rad, lambda _: lon_max_rad)\n",
    "            integral_Y_X, _ = dblquad(\n",
    "                lambda lon, lat: np.cos(lat)*np.cos(lon)*np.cos(lat), lat_min_rad, lat_max_rad,\n",
    "                lambda _: lon_min_rad, lambda _: lon_max_rad)\n",
    "            integral_Y_Y, _ = dblquad(\n",
    "                lambda lon, lat: np.cos(lat)*np.sin(lon)*np.cos(lat), lat_min_rad, lat_max_rad,\n",
    "                lambda _: lon_min_rad, lambda _: lon_max_rad)\n",
    "            integral_Y_Z, _ = dblquad(\n",
    "                lambda lon, lat: np.sin(lat)*np.cos(lat), lat_min_rad, lat_max_rad,\n",
    "                lambda _: lon_min_rad, lambda _: lon_max_rad)\n",
    "            \n",
    "            tile_cache[(topLeftLat, topLeftLon)] = (\n",
    "                area,\n",
    "                integral_Y_W,\n",
    "                integral_Y_X,\n",
    "                integral_Y_Y,\n",
    "                integral_Y_Z\n",
    "            )\n",
    "    \n",
    "    print(\"Integral precomputation complete!\")\n",
    "    return tile_cache\n",
    "\n",
    "\n",
    "def get_tile_for_pixel(lat_pixel, lon_pixel, erp_height, erp_width, tile_size_deg=20):\n",
    "    \"\"\"\n",
    "    Get the tile coordinates for a given pixel in the ERP map.\n",
    "    \"\"\"\n",
    "    # Convert pixel to lat/lon\n",
    "    lat = 90 - (lat_pixel / erp_height) * 180\n",
    "    lon = -180 + (lon_pixel / erp_width) * 360\n",
    "    \n",
    "    # Find which tile this belongs to\n",
    "    tile_lat = int(np.floor(lat / tile_size_deg)) * tile_size_deg\n",
    "    tile_lon = int(np.floor(lon / tile_size_deg)) * tile_size_deg\n",
    "    \n",
    "    return (tile_lat, tile_lon)\n",
    "\n",
    "\n",
    "def compute_audio_saliency_heatmap_vectorized(W, X, Y, Z, audio_samplerate, \n",
    "                                              frame_idx, video_fps, \n",
    "                                              erp_height, erp_width, \n",
    "                                              tile_cache,  sample_every_n_frames, \n",
    "                                              numHeatmaps, tile_size_deg=20):\n",
    "    \"\"\"\n",
    "    Compute audio saliency heatmap for a given frame using precomputed tile integrals.\n",
    "    Each tile's saliency is computed once and replicated to all pixels in that tile.\n",
    "    Extracts audio from 2.5 frames before to 2.5 frames after the current frame.\n",
    "    Returns a 2D array of shape (erp_height, erp_width).\n",
    "    \"\"\"\n",
    "    # Time in seconds corresponding to this video frame\n",
    "    time_sec = frame_idx / video_fps\n",
    "    \n",
    "    # Window: 2.5 frames before and 2.5 frames after = 5 frames total\n",
    "    frameWindow = sample_every_n_frames / 2\n",
    "    window_sec = frameWindow / video_fps\n",
    "    \n",
    "    # Convert time to sample indices\n",
    "    center_sample = int(time_sec * audio_samplerate)\n",
    "    half_window_samples = int(window_sec * audio_samplerate)\n",
    "    start = max(center_sample - half_window_samples, 0)\n",
    "    end = min(center_sample + half_window_samples, len(W))\n",
    "    \n",
    "    # Extract waveform slices\n",
    "    W_slice = W[start:end]\n",
    "    X_slice = X[start:end]\n",
    "    Y_slice = Y[start:end]\n",
    "    Z_slice = Z[start:end]\n",
    "    \n",
    "    # Initialize output saliency map\n",
    "    saliency_map = np.zeros((numHeatmaps, erp_height, erp_width))\n",
    "    \n",
    "    numLatTiles = 180 // tile_size_deg\n",
    "    numLonTiles = 360 // tile_size_deg   \n",
    "\n",
    "    # Calculate pixels per tile\n",
    "    pixels_per_tile_lat = erp_height // numLatTiles  # 9 tiles in latitude\n",
    "    pixels_per_tile_lon = erp_width // numLonTiles   # 18 tiles in longitude\n",
    "    \n",
    "    # Iterate through each tile\n",
    "    for lat_tile in range(numLatTiles):\n",
    "        for lon_tile in range(numLonTiles):\n",
    "            # Get tile coordinates\n",
    "            tile_lat = 90 - lat_tile * tile_size_deg\n",
    "            tile_lon = -180 + lon_tile * tile_size_deg\n",
    "            tile_coords = (tile_lat, tile_lon)\n",
    "            \n",
    "            if tile_coords in tile_cache:\n",
    "                area, integral_Y_W, integral_Y_X, integral_Y_Y, integral_Y_Z = tile_cache[tile_coords]\n",
    "                \n",
    "                # Reconstruct waveform for this tile\n",
    "                wave = (integral_Y_W * W_slice + \n",
    "                       integral_Y_X * X_slice +\n",
    "                       integral_Y_Y * Y_slice + \n",
    "                       integral_Y_Z * Z_slice) / area\n",
    "                \n",
    "                if len(wave) > 0:\n",
    "                    saliency_values = processWave(wave, audio_samplerate)\n",
    "                else:\n",
    "                    saliency_values = np.zeros(numHeatmaps)\n",
    "                \n",
    "                # Fill all pixels in this tile with the same saliency value\n",
    "                y_start = lat_tile * pixels_per_tile_lat\n",
    "                y_end = (lat_tile + 1) * pixels_per_tile_lat\n",
    "                x_start = lon_tile * pixels_per_tile_lon\n",
    "                x_end = (lon_tile + 1) * pixels_per_tile_lon\n",
    "                \n",
    "                # weirdly works cause of python's funky mapping\n",
    "                saliency_map[:, y_start:y_end, x_start:x_end] = saliency_values[:, np.newaxis, np.newaxis]\n",
    "            else:\n",
    "                raise IndexError(\"IDk how we got this.\")\n",
    "    \n",
    "    return saliency_map\n",
    "\n",
    "\n",
    "def normalize_heatmaps(heatmaps):\n",
    "    \"\"\"Normalize heatmap to [0, 1] range.\"\"\"\n",
    "    # returns a list of mins and maxs for each heatmap\n",
    "    h_mins = np.min(heatmaps, axis=(1, 2), keepdims=True)\n",
    "    h_maxs = np.max(heatmaps, axis=(1, 2), keepdims=True)\n",
    "\n",
    "    return (heatmaps - h_mins) / (h_maxs - h_mins)\n",
    "\n",
    "\n",
    "def getFrame(cap, output_width, output_height, frame_idx):    \n",
    "    \"\"\"\n",
    "    Read video and yield resized frames.\n",
    "    \"\"\"\n",
    "    \n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "    ret, frame = cap.read()        \n",
    "    resized_frame = cv2.resize(frame, (output_width, output_height), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    return resized_frame\n",
    "\n",
    "def process_360_video(video_path, audio_path, output_path, \n",
    "                      erp_height=1920, erp_width=3840, sample_every_n_frames=5, numHeatmaps=7):\n",
    "    \"\"\"\n",
    "    Main pipeline to process a 360 video and extract audio saliency heatmaps.\n",
    "    \n",
    "    Parameters:\n",
    "        video_path: path to ERP format 360 video\n",
    "        audio_path: path to first-order ambisonic audio file\n",
    "        output_path: where to save the output .npy file\n",
    "        erp_height: height of ERP format (pixels)\n",
    "        erp_width: width of ERP format (pixels)\n",
    "        sample_every_n_frames: sample every N frames\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load audio\n",
    "    print(\"Loading ambisonic audio...\")\n",
    "    audio_data, audio_samplerate = sf.read(audio_path)\n",
    "    \n",
    "    # Check for 4 channels\n",
    "    if len(audio_data.shape) == 1:\n",
    "        raise ValueError(f\"Audio is mono. Expected 4-channel first-order ambisonics.\")\n",
    "    elif audio_data.shape[1] != 4:\n",
    "        raise ValueError(f\"Audio has {audio_data.shape[1]} channels. Expected 4-channel first-order ambisonics (W, X, Y, Z).\")\n",
    "    \n",
    "    # Split into channels\n",
    "    W = audio_data[:, 0]\n",
    "    X = audio_data[:, 1]\n",
    "    Y = audio_data[:, 2]\n",
    "    Z = audio_data[:, 3]\n",
    "    \n",
    "    print(f\"Audio shape: {audio_data.shape}\")\n",
    "    print(f\"Audio sample rate: {audio_samplerate} Hz\")\n",
    "    print(\"Successfully loaded 4-channel first-order ambisonics audio\")\n",
    "    \n",
    "    # Open video to get metadata\n",
    "    print(\"Opening video...\")\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(f\"Could not open video: {video_path}\")\n",
    "    \n",
    "    video_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    video_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    video_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    print(f\"Video FPS: {video_fps}\")\n",
    "    print(f\"Total frames: {total_frames}\")\n",
    "    print(f\"Video dimensions: {video_width}x{video_height}\")\n",
    "    print(f\"Target ERP dimensions: {erp_width}x{erp_height}\")\n",
    "    \n",
    "    # Check if resizing is needed\n",
    "    need_resize = video_width != erp_width or video_height != erp_height\n",
    "    if need_resize:\n",
    "        print(f\"Video will be resized from {video_width}x{video_height} to {erp_width}x{erp_height}\")\n",
    "    \n",
    "    # Precompute integrals for coarse tiles (20x20 degrees)\n",
    "    tile_cache = precompute_integrals(tile_size_deg=20)\n",
    "    \n",
    "    # Calculate number of sampled frames\n",
    "    num_sampled_frames = (total_frames - math.ceil(sample_every_n_frames / 2)) // sample_every_n_frames\n",
    "    \n",
    "    # Initialize output array\n",
    "    output_array = np.zeros((num_sampled_frames, numHeatmaps, erp_height, erp_width), dtype=np.float16)\n",
    "    \n",
    "    print(f\"Output array shape: {output_array.shape}\")\n",
    "    print(f\"Processing {num_sampled_frames} frames...\")\n",
    "        \n",
    "    # Use frame generator (resizes all frames upfront in the stream). Also, only retrieves them one at a time, instead of keeping it all in memory\n",
    "    for sampled_frame_idx in range(num_sampled_frames):\n",
    "        frame_idx = sample_every_n_frames * (sampled_frame_idx + 1)\n",
    "\n",
    "        frame = getFrame(cap, erp_height, erp_width, frame_idx)\n",
    "        \n",
    "        print(f\"Processing frame {frame_idx}/{total_frames} (sample {sampled_frame_idx}/{num_sampled_frames})\")\n",
    "        \n",
    "        # Compute audio saliency heatmap\n",
    "        saliency_heatmaps = compute_audio_saliency_heatmap_vectorized(W, X, Y, Z, audio_samplerate,\n",
    "                                                                        frame_idx, video_fps, \n",
    "                                                                        erp_height, erp_width,\n",
    "                                                                        tile_cache, sample_every_n_frames, \n",
    "                                                                        numHeatmaps, tile_size_deg=20)\n",
    "        \n",
    "        # Normalize heatmap\n",
    "        saliency_heatmaps = normalize_heatmaps(saliency_heatmaps)\n",
    "        \n",
    "        # Store in output array\n",
    "        output_array[sampled_frame_idx] = saliency_heatmaps\n",
    "\n",
    "    cap.release()\n",
    "    \n",
    "    # Save output\n",
    "    print(f\"Saving output to {output_path}...\")\n",
    "    np.save(output_path, output_array)\n",
    "    \n",
    "    print(f\"Done! Output shape: {output_array.shape}\")\n",
    "    print(f\"Saved to: {output_path}\")\n",
    "    \n",
    "    return output_array\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.chdir(\"./../..\")\n",
    "    \n",
    "    # Configuration - modify as needed\n",
    "    ERP_WIDTH = 1920  # width\n",
    "    ERP_HEIGHT = 960  # height\n",
    "    SAMPLE_RATE = 5  # sample every 5 frames\n",
    "    FILE_NAME = \"5020\"\n",
    "    VIDEO_PATH = f\"Data/Pre-Processed-Data/{FILE_NAME}.mp4\"  # ERP format 360 video\n",
    "    AUDIO_PATH = f\"Data/Pre-Processed-Data/{FILE_NAME}.wav\"\n",
    "    OUTPUT_PATH = f\"FinalTrainingData/{FILE_NAME}.npy\"\n",
    "    NUM_HEATMAPS = 7\n",
    "    \n",
    "    # Run the pipeline\n",
    "    saliency_array = process_360_video(VIDEO_PATH, AUDIO_PATH, OUTPUT_PATH,\n",
    "                                      erp_height=ERP_HEIGHT, erp_width=ERP_WIDTH,\n",
    "                                      sample_every_n_frames=SAMPLE_RATE, numHeatmaps=NUM_HEATMAPS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
