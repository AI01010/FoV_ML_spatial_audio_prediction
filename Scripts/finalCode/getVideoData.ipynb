{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a3f0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo path was: c:\\Users\\mahd\\Documents\\FOV Prediction\\Scripts\\finalCode\\../../U-2-Net-Repo\n",
      "Using device: cpu\n",
      "U2 Net Model ready!\n",
      "Successfully imported core RAFT modules from your fork.\n",
      "Loading RAFT cpu model...\n",
      "RAFT model loaded on cpu successfully.\n",
      "Loading ambisonic audio...\n",
      "Audio shape: (2880000, 4)\n",
      "Audio sample rate: 48000 Hz\n",
      "Successfully loaded 4-channel first-order ambisonics audio\n",
      "Opening video...\n",
      "Video FPS: 60.0\n",
      "Total frames: 3598\n",
      "Video dimensions: 3840x1920\n",
      "Video will be resized from 3840x1920 to 1920x960\n",
      "Precomputing integrals for 20° tiles...\n",
      "Integral precomputation complete!\n",
      "Reading CSV file: Data/Pre-Processed-Data/head_data/head_video_0001.csv\n",
      "Output array shape: (3, 9, 960, 1920)\n",
      "Processing 3 frames...\n",
      "Processing frame 5/3598 (sample 0/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mahd\\Documents\\FOV Prediction\\Scripts\\finalCode\\../../U-2-Net-Repo\\model\\u2net.py:23: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n",
      "  src = F.upsample(src,size=tar.shape[2:],mode='bilinear')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trying scale 1.0...\n",
      "Padded tensor shapes: t1=torch.Size([1, 3, 960, 1920]), t2=torch.Size([1, 3, 960, 1920])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mahd\\Documents\\FOV Prediction\\Scripts\\finalCode\\../../Hani-Raft-Repo/core\\raft.py:99: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=self.args.mixed_precision):\n",
      "c:\\Users\\mahd\\Documents\\FOV Prediction\\Scripts\\finalCode\\../../Hani-Raft-Repo/core\\raft.py:110: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=self.args.mixed_precision):\n",
      "c:\\Users\\mahd\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\functional.py:505: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:4319.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "c:\\Users\\mahd\\Documents\\FOV Prediction\\Scripts\\finalCode\\../../Hani-Raft-Repo/core\\raft.py:127: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=self.args.mixed_precision):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Flow computed successfully at scale 1.0\n",
      "Processing frame 10/3598 (sample 1/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mahd\\Documents\\FOV Prediction\\Scripts\\finalCode\\../../U-2-Net-Repo\\model\\u2net.py:23: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n",
      "  src = F.upsample(src,size=tar.shape[2:],mode='bilinear')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trying scale 1.0...\n",
      "Padded tensor shapes: t1=torch.Size([1, 3, 960, 1920]), t2=torch.Size([1, 3, 960, 1920])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mahd\\Documents\\FOV Prediction\\Scripts\\finalCode\\../../Hani-Raft-Repo/core\\raft.py:99: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=self.args.mixed_precision):\n",
      "c:\\Users\\mahd\\Documents\\FOV Prediction\\Scripts\\finalCode\\../../Hani-Raft-Repo/core\\raft.py:110: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=self.args.mixed_precision):\n",
      "c:\\Users\\mahd\\Documents\\FOV Prediction\\Scripts\\finalCode\\../../Hani-Raft-Repo/core\\raft.py:127: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=self.args.mixed_precision):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Flow computed successfully at scale 1.0\n",
      "Processing frame 15/3598 (sample 2/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mahd\\Documents\\FOV Prediction\\Scripts\\finalCode\\../../U-2-Net-Repo\\model\\u2net.py:23: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n",
      "  src = F.upsample(src,size=tar.shape[2:],mode='bilinear')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trying scale 1.0...\n",
      "Padded tensor shapes: t1=torch.Size([1, 3, 960, 1920]), t2=torch.Size([1, 3, 960, 1920])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mahd\\Documents\\FOV Prediction\\Scripts\\finalCode\\../../Hani-Raft-Repo/core\\raft.py:99: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=self.args.mixed_precision):\n",
      "c:\\Users\\mahd\\Documents\\FOV Prediction\\Scripts\\finalCode\\../../Hani-Raft-Repo/core\\raft.py:110: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=self.args.mixed_precision):\n",
      "c:\\Users\\mahd\\Documents\\FOV Prediction\\Scripts\\finalCode\\../../Hani-Raft-Repo/core\\raft.py:127: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=self.args.mixed_precision):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Flow computed successfully at scale 1.0\n",
      "\n",
      "Generated 3 tile indices\n",
      "Tile index range: 74 to 74\n",
      "Saving output to FinalTrainingData/0001_heatmaps.npy...\n",
      "Done! Output shape: (3, 9, 960, 1920)\n",
      "Saved to: FinalTrainingData/0001_labels.npy\n",
      "Saving output to FinalTrainingData/0001_heatmaps.npy...\n",
      "Done! Output shape: (3, 9, 960, 1920)\n",
      "Saved to: FinalTrainingData/0001_heatmaps.npy\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from getAudioSaliency import compute_audio_saliency_heatmap_vectorized, precompute_integrals\n",
    "from getVideoLabels import filterDf, uv_to_tile_index\n",
    "from getVideoSaliency import compute_video_saliency_heatmap_vectorized\n",
    "\n",
    "def normalize_heatmaps(heatmaps):\n",
    "    \"\"\"Normalize heatmap to [0, 1] range.\"\"\"\n",
    "    # returns a list of mins and maxs for each heatmap\n",
    "    h_mins = np.min(heatmaps, axis=(1, 2), keepdims=True)\n",
    "    h_maxs = np.max(heatmaps, axis=(1, 2), keepdims=True)\n",
    "\n",
    "    return (heatmaps - h_mins) / (h_maxs - h_mins)\n",
    "\n",
    "\n",
    "def getFrame(cap, output_height, output_width, frame_idx):    \n",
    "    \"\"\"\n",
    "    Read video and yield resized frames.\n",
    "    \"\"\"\n",
    "    \n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "    ret, frame = cap.read()        \n",
    "    resized_frame = cv2.resize(frame, (output_width, output_height), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "\n",
    "    return resized_frame\n",
    "\n",
    "def process_360_video(video_name, video_path, audio_path, outputX_path, outputY_path,\n",
    "                      csv_path, erp_height=1920, erp_width=3840, \n",
    "                      sample_every_n_frames=5, numHeatmaps=7, participant_id = 1,\n",
    "                      cols = 16, rows = 9):\n",
    "    \"\"\"\n",
    "    Main pipeline to process a 360 video and extract audio saliency heatmaps.\n",
    "    \n",
    "    Parameters:\n",
    "        video_path: path to ERP format 360 video\n",
    "        audio_path: path to first-order ambisonic audio file\n",
    "        output_path: where to save the output .npy file\n",
    "        erp_height: height of ERP format (pixels)\n",
    "        erp_width: width of ERP format (pixels)\n",
    "        sample_every_n_frames: sample every N frames\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load audio\n",
    "    print(\"Loading ambisonic audio...\")\n",
    "    audio_data, audio_samplerate = sf.read(audio_path)\n",
    "    \n",
    "    # Check for 4 channels\n",
    "    if len(audio_data.shape) == 1:\n",
    "        raise ValueError(f\"Audio is mono. Expected 4-channel first-order ambisonics.\")\n",
    "    elif audio_data.shape[1] != 4:\n",
    "        raise ValueError(f\"Audio has {audio_data.shape[1]} channels. Expected 4-channel first-order ambisonics (W, X, Y, Z).\")\n",
    "    \n",
    "    # Split into channels\n",
    "    W = audio_data[:, 0]\n",
    "    X = audio_data[:, 1]\n",
    "    Y = audio_data[:, 2]\n",
    "    Z = audio_data[:, 3]\n",
    "    \n",
    "    print(f\"Audio shape: {audio_data.shape}\")\n",
    "    print(f\"Audio sample rate: {audio_samplerate} Hz\")\n",
    "    print(\"Successfully loaded 4-channel first-order ambisonics audio\")\n",
    "    \n",
    "    # Open video to get metadata\n",
    "    print(\"Opening video...\")\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(f\"Could not open video: {video_path}\")\n",
    "    \n",
    "    video_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    video_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    video_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    print(f\"Video FPS: {video_fps}\")\n",
    "    print(f\"Total frames: {total_frames}\")\n",
    "    print(f\"Video dimensions: {video_width}x{video_height}\")\n",
    "    \n",
    "    # Check if resizing is needed\n",
    "    need_resize = video_width != erp_width or video_height != erp_height\n",
    "    if need_resize:\n",
    "        print(f\"Video will be resized from {video_width}x{video_height} to {erp_width}x{erp_height}\")\n",
    "    \n",
    "    # Precompute integrals for coarse tiles (20x20 degrees)\n",
    "    tile_cache = precompute_integrals(tile_size_deg=20)\n",
    "    \n",
    "    # Calculate number of sampled frames\n",
    "    # num_sampled_frames = (total_frames - math.ceil(sample_every_n_frames / 2)) // sample_every_n_frames\n",
    "    num_sampled_frames = 3\n",
    "\n",
    "    labelDf = filterDf(csv_path, participant_id, video_name)\n",
    "    tile_indices = []\n",
    "    \n",
    "    # Initialize output array\n",
    "    output_array = np.zeros((num_sampled_frames, numHeatmaps, erp_height, erp_width), dtype=np.float16)\n",
    "    \n",
    "    print(f\"Output array shape: {output_array.shape}\")\n",
    "    print(f\"Processing {num_sampled_frames} frames...\")\n",
    "        \n",
    "    # Use frame generator (resizes all frames upfront in the stream). Also, only retrieves them one at a time, instead of keeping it all in memory\n",
    "    for sampled_frame_idx in range(num_sampled_frames):\n",
    "        frame_idx = sample_every_n_frames * (sampled_frame_idx + 1)\n",
    "\n",
    "        prevFrame = getFrame(cap, erp_height, erp_width, frame_idx - 1)\n",
    "        frame = getFrame(cap, erp_height, erp_width, frame_idx)\n",
    "        \n",
    "        print(f\"Processing frame {frame_idx}/{total_frames} (sample {sampled_frame_idx}/{num_sampled_frames})\")\n",
    "        \n",
    "        # Compute audio saliency heatmap\n",
    "        saliency_heatmaps = np.concatenate([compute_audio_saliency_heatmap_vectorized(W, X, Y, Z, audio_samplerate,\n",
    "                                                                        frame_idx, video_fps,\n",
    "                                                                        erp_height, erp_width,\n",
    "                                                                        tile_cache, sample_every_n_frames,\n",
    "                                                                        numHeatmaps-2, tile_size_deg=20),\n",
    "                                                                        compute_video_saliency_heatmap_vectorized(prevFrame, frame, frame_idx, video_fps,\n",
    "                                                                                                            erp_height, erp_width,\n",
    "                                                                                                            tile_cache, sample_every_n_frames,\n",
    "                                                                                                            numHeatmaps-7, tile_size_deg=20)],\n",
    "                                                                        axis=0\n",
    "                                      )\n",
    "        \n",
    "        # Normalize heatmap\n",
    "        saliency_heatmaps = normalize_heatmaps(saliency_heatmaps)\n",
    "        \n",
    "        # Store in output array\n",
    "        output_array[sampled_frame_idx] = saliency_heatmaps\n",
    "\n",
    "        targetIndex = frame_idx / video_fps\n",
    "\n",
    "         # Find the row with closest timestamp\n",
    "        idx = (labelDf['t'] - targetIndex).abs().idxmin()\n",
    "\n",
    "        # Get u, v coordinates\n",
    "        u = labelDf.loc[idx, 'u']\n",
    "        v = labelDf.loc[idx, 'v']\n",
    "\n",
    "        # Convert to tile index\n",
    "        tile_idx = uv_to_tile_index(u, v, rows, cols)\n",
    "        tile_indices.append(tile_idx)\n",
    "    \n",
    "    tile_indices_array = np.array(tile_indices)\n",
    "\n",
    "    print(f\"\\nGenerated {len(tile_indices_array)} tile indices\")\n",
    "    print(f\"Tile index range: {tile_indices_array.min()} to {tile_indices_array.max()}\")\n",
    "\n",
    "    # checks if this is running in google colab\n",
    "    try:\n",
    "        import google.colab.files as gfiles\n",
    "        IN_COLAB = True\n",
    "    except ImportError:\n",
    "        IN_COLAB = False\n",
    "\n",
    "\n",
    "    print(f\"Saving output to {outputX_path}...\")\n",
    "    np.save(outputY_path, tile_indices_array)\n",
    "    print(f\"Done! Output shape: {output_array.shape}\")\n",
    "    print(f\"Saved to: {outputY_path}\")\n",
    "\n",
    "    \n",
    "    # Save output\n",
    "    print(f\"Saving output to {outputX_path}...\")\n",
    "    np.save(outputX_path, output_array)\n",
    "    print(f\"Done! Output shape: {output_array.shape}\")\n",
    "    print(f\"Saved to: {outputX_path}\")\n",
    "\n",
    "    if(IN_COLAB):\n",
    "        gfiles.download(outputX_path)\n",
    "        gfiles.download(outputY_path)\n",
    "\n",
    "    cap.release()\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    os.chdir(\"./../..\")\n",
    "    \n",
    "    # Configuration - modify as needed\n",
    "    ERP_WIDTH = 1920  # width\n",
    "    ERP_HEIGHT = 960  # height\n",
    "    SAMPLE_RATE = 5  # sample every 5 frames\n",
    "    FILE_NAME = \"0001\"\n",
    "    \n",
    "    VIDEO_PATH = f\"Data/Pre-Processed-Data/{FILE_NAME}/{FILE_NAME}_mono_60fps.mp4\"  # ERP format 360 video\n",
    "    AUDIO_PATH = f\"Data/Pre-Processed-Data/{FILE_NAME}/{FILE_NAME}.wav\"\n",
    "    INPUT_CSV_PATH = f\"Data/Pre-Processed-Data/head_data/head_video_{FILE_NAME}.csv\"\n",
    "    OUTPUT_X_PATH = f\"FinalTrainingData/{FILE_NAME}_heatmaps.npy\"\n",
    "    OUTPUT_Y_PATH = f\"FinalTrainingData/{FILE_NAME}_labels.npy\"\n",
    "    NUM_HEATMAPS = 9\n",
    "    PARTICIPANT_ID = 11\n",
    "    TILE_COLS = 16\n",
    "    TILE_ROWS = 9\n",
    "\n",
    "    \n",
    "    # Run the pipeline\n",
    "    process_360_video(FILE_NAME, VIDEO_PATH, AUDIO_PATH, OUTPUT_X_PATH, OUTPUT_Y_PATH, INPUT_CSV_PATH,\n",
    "                                      erp_height=ERP_HEIGHT, erp_width=ERP_WIDTH,\n",
    "                                      sample_every_n_frames=SAMPLE_RATE, numHeatmaps=NUM_HEATMAPS,\n",
    "                                      participant_id = PARTICIPANT_ID, cols=TILE_COLS, rows=TILE_ROWS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
