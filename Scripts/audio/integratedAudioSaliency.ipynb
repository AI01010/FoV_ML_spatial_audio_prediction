{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a0cabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import librosa  \n",
    "\n",
    "# Load the .wav file\n",
    "data, samplerate = sf.read(\"ambisonic_audio.wav\")\n",
    "\n",
    "def compute_audio_at_direction(W: np.ndarray, X: np.ndarray, \n",
    "                                   Y: np.ndarray, Z: np.ndarray,\n",
    "                                   theta: float, phi: float) -> float:\n",
    "    \"\"\"\n",
    "    Compute audio waveform/energy at a specific spherical direction.\n",
    "    Uses the spherical harmonic reconstruction formula:\n",
    "    s(θ, φ, t) = W(t)Y_W(θ,φ) + X(t)Y_X(θ,φ) + Y(t)Y_Y(θ,φ) + Z(t)Y_Z(θ,φ)\n",
    "    \n",
    "    Args:\n",
    "        W, X, Y, Z: Ambisonic channel waveforms for this time frame\n",
    "        theta: Azimuth angle (longitude) in radians\n",
    "        phi: Elevation angle (latitude) in radians\n",
    "        \n",
    "    Returns:\n",
    "        energy: Audio energy at this direction\n",
    "    \"\"\"\n",
    "    # Spherical harmonic basis functions (First-Order Ambisonics)\n",
    "    # Using ACN/SN3D convention\n",
    "    Y_W = 1.0                                    # Y_0^0 (omnidirectional)\n",
    "    Y_X = np.cos(phi) * np.cos(theta)           # Y_1^1 (front-back)\n",
    "    Y_Y = np.cos(phi) * np.sin(theta)           # Y_1^-1 (left-right)\n",
    "    Y_Z = np.sin(phi)                            # Y_1^0 (up-down)\n",
    "    \n",
    "    # Reconstruct audio signal at this direction\n",
    "    signal = W * Y_W + X * Y_X + Y * Y_Y + Z * Y_Z\n",
    "    \n",
    "    # Compute energy (RMS)\n",
    "    energy = np.sqrt(np.mean(signal ** 2))\n",
    "    \n",
    "    return energy\n",
    "\n",
    "longitude = 30\n",
    "latitude = 67\n",
    "    \n",
    "print(f\"Shape: {data.shape}\")   # (num_samples, 4)\n",
    "print(f\"Sample rate: {samplerate} Hz\")\n",
    "\n",
    "# Split into channels\n",
    "W = data[:, 0]\n",
    "X = data[:, 1]\n",
    "Y = data[:, 2]\n",
    "Z = data[:, 3]\n",
    "\n",
    "\n",
    "def processWave(type, wave, sampleRate):\n",
    "    windowSize = 2048\n",
    "    hopSize = 100\n",
    "    \n",
    "    # converts this to a Short-Time Fourier Transform. Tells you how much eergy has at each frequency over time.\n",
    "    # does this by going through windows. Length of each window defined by n_fft. Then, shifts window to right by length\n",
    "    # hop length. at each window, computes how much of each frequency is present.\n",
    "    # final value is 2D array of rows being each frequency, columns being time (which is now the windows), so value being amplitude/energy for that time and frequency\n",
    "    stftWave = np.abs(librosa.stft(wave, n_fft=windowSize, hop_length=hopSize))\n",
    "    # when we get the mel, that just converts all the frequencies to 128 possible onces, which are moreso frequencies humans can hear. So compressing\n",
    "    # the frequencies from a large number of frequencies to a smaller number, in this case n_mels amount\n",
    "    mel = librosa.feature.melspectrogram(S = stftWave, sr= sampleRate, n_mels = 128)\n",
    "    # converts from power scaling of audio to decibel scaling, cause humans perceive in moreso logarithm of audio (so higher sounds kinda taper off to us)\n",
    "    logMel = librosa.power_to_db(mel, ref=np.max)\n",
    "\n",
    "    # gets overall frame energy, including amplitude\n",
    "    frameEnergy = np.sqrt(np.mean(logMel ** 2, axis=0))\n",
    "    # gets the contrast in energy between frequencies within a specific frequency band, so where some frequencies bands may have parts of high energy frequencies, while other parts are low energy\n",
    "    contrast = librosa.feature.spectral_contrast(S = stftWave, sr=sampleRate)\n",
    "    # combines the difference frequency bands to get a average contrast for that time frame\n",
    "    contrast = np.mean(contrast, axis=0)\n",
    "    # basically gets how much the sound chagnes over time. Does this by getting differnece over time fimes with np.diff, squaring that value, and getting its sum\n",
    "    temporal_novelty = np.sum(np.diff(logMel, axis=1) ** 2, axis=0)\n",
    "    # do this to add an extra value cause rn, the length is T - 1, since you're getting difference between frames. So add 1 to get it to T length\n",
    "    temporal_novelty = np.insert(temporal_novelty, 0, 0)\n",
    "\n",
    "    # standard normalization\n",
    "    frameEnergyNorm = frameEnergy / np.max(frameEnergy)\n",
    "    contrastNorm = contrast / np.max(contrast)\n",
    "    temporal_novelty_norm = temporal_novelty / np.max(temporal_novelty)\n",
    "\n",
    "    # gets it for the overall time\n",
    "    finalSaliency = 0.5 * frameEnergyNorm + 0.3 * contrastNorm + 0.2 * temporal_novelty_norm\n",
    "\n",
    "    secondsIn = 6.5\n",
    "    # divide by hoSize as first secondIn * sr gets the specific sample we want. Dividing by hopSize tells us how many windows to traverse\n",
    "    # to get to that sample, as say if it's less than hopSize, its in the window at index 0, if it's slightly more, it's in the window at index 1, etc\n",
    "    index = int(secondsIn * sampleRate / hopSize)\n",
    "\n",
    "\n",
    "    print(f\"For type of wave {type}, saliency was {finalSaliency[index]}\")\n",
    "\n",
    "\n",
    "\n",
    "processWave(\"Example\", wave, samplerate)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
