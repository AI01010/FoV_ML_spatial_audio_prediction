{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Iteration 1"
      ],
      "metadata": {
        "id": "ZLZhoElJQBor"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zffLFSKJPyU7"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "from IPython.display import Image\n",
        "\n",
        "#Importing the image from your files\n",
        "uploaded = files.upload()\n",
        "\n",
        "img_name = list(uploaded.keys())[0]\n",
        "display(Image(filename=img_name))\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "\n",
        "from PIL import Image as PILImage\n",
        "\n",
        "img_name = \"Ocean360Picture.jpg\"\n",
        "\n",
        "img = PILImage.open(img_name)\n",
        "img_matrix = np.array(img)\n",
        "rows,cols,channel = img_matrix.shape\n",
        "print(f\"Original image shape:\", img_matrix.shape)  # (rows, cols, color_channels)\n",
        "display(Image(filename=img_name))\n",
        "\n",
        "num_tiles = 36\n",
        "tile_height = rows // num_tiles\n",
        "tile_width = cols // num_tiles\n",
        "\n",
        "tiles = []  # will store all tiles\n",
        "for i in range(num_tiles):\n",
        "    row_tiles = []\n",
        "    for j in range(num_tiles):\n",
        "        # extract tile (i,j)\n",
        "        tile = img_matrix[\n",
        "            i * tile_height:(i + 1) * tile_height,\n",
        "            j * tile_width:(j + 1) * tile_width\n",
        "        ]\n",
        "        row_tiles.append(tile)\n",
        "    tiles.append(row_tiles)\n",
        "\n",
        "\n",
        "plt.imshow(tiles[20][20])\n",
        "plt.title(\"Tile (20,20)\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "rows_combined = [np.hstack(row) for row in tiles]\n",
        "reconstructed = np.vstack(rows_combined)\n",
        "\n",
        "plt.imshow(reconstructed)\n",
        "plt.title(\"Reconstructed 360° Image from 36x36 Tiles\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "num_tiles = 36\n",
        "tile_height = reconstructed.shape[0] // num_tiles\n",
        "tile_width = reconstructed.shape[1] // num_tiles\n",
        "\n",
        "# Draw vertical lines\n",
        "for x in range(0, reconstructed.shape[1], tile_width):\n",
        "    plt.vlines(x, 0, reconstructed.shape[0], color='red', linewidth=0.5, alpha=0.6)\n",
        "\n",
        "# Draw horizontal lines\n",
        "for y in range(0, reconstructed.shape[0], tile_height):\n",
        "    plt.hlines(y, 0, reconstructed.shape[1], color='red', linewidth=0.5, alpha=0.6)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iteration 2"
      ],
      "metadata": {
        "id": "MjSEIPc-QXw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# 360° ERP Image → Spherical Projection Pipeline\n",
        "# Steps 1–5\n",
        "# ======================================\n",
        "\n",
        "# STEP 1: Add and display image\n",
        "\n",
        "# from google.colab import files\n",
        "\n",
        "from IPython.display import Image as ColabImage\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# Upload ERP image (e.g., Ocean360Picture.jpg)\n",
        "\n",
        "# uploaded = files.upload()\n",
        "# filename = list(uploaded.keys())[0]\n",
        "# ColabImage(filename)\n",
        "\n",
        "filename = \"Ocean360Picture.jpg\"\n",
        "\n",
        "# Read image in RGB format\n",
        "erp = cv2.imread(filename)\n",
        "erp = cv2.cvtColor(erp, cv2.COLOR_BGR2RGB)\n",
        "H, W, _ = erp.shape\n",
        "\n",
        "print(f\"Height is {H}, and width is {W}\")\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.imshow(erp)\n",
        "plt.title(\"Step 1: Original ERP (Equirectangular Projection) Image\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "# ======================================\n",
        "# STEP 2: Create 36x36 Matrix Placeholder\n",
        "# ======================================\n",
        "num_tiles = 36\n",
        "tile_matrix = [[None for _ in range(num_tiles)] for _ in range(num_tiles)]\n",
        "print(f\"Step 2: Created {num_tiles}x{num_tiles} matrix for spherical tiles.\")\n",
        "\n",
        "# ======================================\n",
        "# STEP 3: Reconstruct Image as Sphere\n",
        "# ======================================\n",
        "# Create meshgrid of longitude (λ) and latitude (φ)\n",
        "u = np.linspace(0, W - 1, num_tiles)\n",
        "v = np.linspace(0, H - 1, num_tiles)\n",
        "U, V = np.meshgrid(u, v)\n",
        "\n",
        "# Convert ERP pixel coords → spherical angles\n",
        "lam = 2 * np.pi * (U / W - 0.5)       # longitude [-π, π]\n",
        "phi = np.pi * (0.5 - V / H)           # latitude [π/2, -π/2]\n",
        "\n",
        "# Convert spherical → Cartesian coords (X,Y,Z)\n",
        "X = np.cos(phi) * np.cos(lam)\n",
        "Y = np.sin(phi)\n",
        "Z = np.cos(phi) * np.sin(lam)\n",
        "\n",
        "# Sample colors from ERP for each spherical coordinate\n",
        "U_int = np.clip(U.astype(int), 0, W - 1)\n",
        "V_int = np.clip(V.astype(int), 0, H - 1)\n",
        "colors = erp[V_int, U_int, :] / 255.0  # normalize to [0,1] for matplotlib\n",
        "\n",
        "# Plot 3D sphere\n",
        "fig = plt.figure(figsize=(10,10))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot_surface(X, Y, Z, facecolors=colors, rstride=1, cstride=1, linewidth=0, antialiased=False)\n",
        "ax.set_box_aspect([1,1,1])\n",
        "ax.set_title(\"Step 3: Seamless 360° Spherical Reconstruction (36×36 Grid)\")\n",
        "ax.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "# ======================================\n",
        "# STEP 4: Split ERP into Spherical Tiles\n",
        "# ======================================\n",
        "delta_lambda = 2 * np.pi / num_tiles   # longitude step\n",
        "delta_phi = np.pi / num_tiles          # latitude step\n",
        "\n",
        "for j in range(num_tiles):\n",
        "    for i in range(num_tiles):\n",
        "        # Tile angular bounds\n",
        "        lam_min = -np.pi + i * delta_lambda\n",
        "        lam_max = -np.pi + (i + 1) * delta_lambda\n",
        "        phi_max =  np.pi/2 - j * delta_phi\n",
        "        phi_min =  np.pi/2 - (j + 1) * delta_phi\n",
        "\n",
        "        # Convert angular bounds → ERP pixel coordinates\n",
        "        u_min = int(((lam_min + np.pi) / (2 * np.pi)) * W)\n",
        "        u_max = int(((lam_max + np.pi) / (2 * np.pi)) * W)\n",
        "        v_min = int(((np.pi/2 - phi_max) / np.pi) * H)\n",
        "        v_max = int(((np.pi/2 - phi_min) / np.pi) * H)\n",
        "\n",
        "        # Clip to valid image range\n",
        "        u_min, u_max = np.clip([u_min, u_max], 0, W)\n",
        "        v_min, v_max = np.clip([v_min, v_max], 0, H)\n",
        "\n",
        "        # Extract tile region from ERP\n",
        "        tile_matrix[j][i] = erp[v_min:v_max, u_min:u_max, :]\n",
        "\n",
        "print(\"Step 4: Image successfully divided into 36×36 spherical tiles.\")\n",
        "\n",
        "# Quick check: display one random tile\n",
        "test_i, test_j = np.random.randint(0, num_tiles, 2)\n",
        "tile = tile_matrix[test_j][test_i]\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.imshow(tile)\n",
        "plt.title(f\"Sample Tile [{test_j}, {test_i}] (Lat/Long patch on sphere)\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "# ======================================\n",
        "# STEP 5: ERP Image with Non-Distorted Grid Lines (Equal Spherical Angles)\n",
        "# ======================================\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12,6))\n",
        "ax.imshow(erp)\n",
        "ax.set_title(\"Step 5: ERP Image with Spherical-Angle Grid (36×36)\")\n",
        "ax.axis(\"off\")\n",
        "\n",
        "# Define tile counts\n",
        "num_tiles = 36\n",
        "\n",
        "# Draw vertical lines (longitude) — evenly spaced in λ\n",
        "for i in range(1, num_tiles):\n",
        "    x = i * (W / num_tiles)\n",
        "    ax.plot([x, x], [0, H], color='red', linewidth=0.7)\n",
        "\n",
        "# Draw horizontal lines (latitude) — spaced by equal φ angles\n",
        "phi_vals = np.linspace(np.pi/2, -np.pi/2, num_tiles + 1)  # 36 equal latitude divisions\n",
        "y_positions = H * (0.5 - (phi_vals / np.pi))  # convert φ → ERP pixel coordinate\n",
        "\n",
        "for y in y_positions:\n",
        "    ax.plot([0, W], [y, y], color='red', linewidth=0.7)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "for j in range(num_tiles):\n",
        "    y_min = int(y_positions[j])\n",
        "    y_max = int(y_positions[j+1])\n",
        "    pixel_height = y_max - y_min\n",
        "    print(f\"Tile {j}: pixel height = {pixel_height}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "-MZIOVATQyPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iteration 3"
      ],
      "metadata": {
        "id": "p6OewkxkQ39Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# 360° ERP Image → Spherical Projection Pipeline\n",
        "# Steps 1–5\n",
        "# ======================================\n",
        "\n",
        "# STEP 1: Add and display image\n",
        "\n",
        "# from google.colab import files\n",
        "\n",
        "from IPython.display import Image as ColabImage\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import math\n",
        "\n",
        "# Upload ERP image (e.g., Ocean360Picture.jpg)\n",
        "\n",
        "# uploaded = files.upload()\n",
        "# filename = list(uploaded.keys())[0]\n",
        "# ColabImage(filename)\n",
        "\n",
        "filename = \"Street360Picture.jpg\"\n",
        "\n",
        "# Read image in RGB format\n",
        "erp = cv2.imread(filename)\n",
        "erp = cv2.cvtColor(erp, cv2.COLOR_BGR2RGB)\n",
        "H, W, _ = erp.shape\n",
        "\n",
        "print(f\"Height is {H}, and width is {W}\")\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.imshow(erp)\n",
        "plt.title(\"Step 1: Original ERP (Equirectangular Projection) Image\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "# Unlike simply slicing a rectangular region from the original ERP image, which causes distortion\n",
        "# (especially near the poles), this method defines each tile directly on the sphere using uniform\n",
        "# latitude and longitude steps.\n",
        "#\n",
        "# It also differs from a forward approach where each ERP pixel is projected to the sphere and\n",
        "# then back to ERP for each tile, which is more complex and can leave gaps.\n",
        "#\n",
        "# Here, each tile pixel is reverse-mapped to fractional coordinates in the original ERP image and\n",
        "# sampled using interpolation. This ensures angular uniformity, reduces distortion, and produces\n",
        "# rectified tiles that accurately represent the spherical surface.\n",
        "\n",
        "widthDegree = 20\n",
        "heightDegree = 20\n",
        "\n",
        "num_lon_tiles = int(360 / widthDegree)  # 18\n",
        "num_lat_tiles = int(180 / heightDegree)  # 9\n",
        "\n",
        "# we first decide on our tile width and derive height from it\n",
        "\n",
        "tileWidth = int(round(W / num_lon_tiles))\n",
        "tileHeight = int(round(H / num_lat_tiles))\n",
        "\n",
        "# tileWidth = int(W / num_lon_tiles)\n",
        "# tileHeight = int(round((H/W * tileWidth)))\n",
        "\n",
        "print(f\"Wdith will be {tileWidth}, height will be {tileHeight}\")\n",
        "\n",
        "# this will be the array of new tiles, where instead of each element being a list of pixels, its the actual image. same indices though\n",
        "finalTiles = [[None for _ in range(num_lon_tiles)] for _ in range(num_lat_tiles)]\n",
        "\n",
        "# === Step 5: Loop over each tile (lat × lon grid) ===\n",
        "for lat_i in range(num_lat_tiles):\n",
        "    # Latitude boundaries for this tile\n",
        "    lat_max = 90.0 - lat_i * heightDegree    # top edge (starts from +90° at north pole)\n",
        "    lat_min = lat_max - heightDegree         # bottom edge, go a single step down\n",
        "\n",
        "    # this returns an array of specific latitude values, where length of array equals tileHeight. Basically, for each\n",
        "    # pixel, we get the corresponding lattitude.\n",
        "    lat_vals = np.linspace(lat_max, lat_min, tileHeight)\n",
        "\n",
        "    # now, loop through each longitude (horizontally). Now can actually access each tile\n",
        "    for lon_j in range(num_lon_tiles):\n",
        "\n",
        "        # Longitude boundaries for this tile\n",
        "        lon_min = lon_j * widthDegree - 180.0\n",
        "        # to get max, go one stepi n this direction\n",
        "        lon_max = lon_min + widthDegree\n",
        "\n",
        "        # Uniform sampling horizontally: left to right. For each pixel, we get the corresponding longitude\n",
        "        lon_vals = np.linspace(lon_min, lon_max, tileWidth)\n",
        "\n",
        "        # === Step 7: Convert lat/lon → ERP pixel coordinates. Note these will be fractions. You're basically sampling parts of the sphere and projecting them, rather than\n",
        "        # projecting the original pixels.\n",
        "        # erp_x and erp_y as 1D vectors\n",
        "        erp_x_vec = (lon_vals + 180.0) / 360.0 * (W - 1)   # shape: (tileWidth,)\n",
        "        erp_y_vec = (90.0 - lat_vals) / 180.0 * (H - 1)    # shape: (tileHeight,)\n",
        "\n",
        "        # broadcast to 2D arrays on the fly. We need to do this basically because cv2.remap expects a 2D matrix for each map, which is used for say\n",
        "        # non rectangular tilling or sampling where map_x and map_y aren't of the same dimesnion, so kinda non uniform sampling. However since we do\n",
        "        # do it uniformly, we can keep it a vector up until the point we pass it into our model. It's basically expecting a matrix where say for map_x,\n",
        "        # map_x[i][j] is at horizontal pixel x and veftical pixel j, what's the longitude. Or after beign converted, what's the fracitonal decimal point  corresponging\n",
        "        # to that longitude in the original image. Same idea with map_y\n",
        "\n",
        "        # np.tile repeats the vector into multiple rows. So copies current row tileHeight times.\n",
        "\n",
        "        # np.repeat changes it so that each value in the inner vector is the same, but now each row has a different number\n",
        "        # from the other rows. By repeating across axis 1, means we repeat each inner vector\n",
        "        map_x = np.tile(erp_x_vec, (tileHeight, 1)).astype(np.float32)  # (tileHeight, tileWidth)\n",
        "        map_y = np.repeat(erp_y_vec[:, np.newaxis], tileWidth, axis=1).astype(np.float32)\n",
        "\n",
        "        # Each output pixel tile_img[i,j] samples the original ERP at coordinates (map_x[i,j], map_y[i,j]).\n",
        "        # map_x and map_y are floating-point, so remap uses interpolation to compute the pixel value.\n",
        "        # BORDER_WRAP ensures horizontal wrapping at longitude ±180°, preserving continuity in 360° images.\n",
        "\n",
        "        tile_img = cv2.remap(\n",
        "            erp, map_x, map_y,\n",
        "            interpolation=cv2.INTER_LINEAR,\n",
        "            borderMode=cv2.BORDER_WRAP\n",
        "        )\n",
        "\n",
        "        # Save this tile\n",
        "        finalTiles[lat_i][lon_j] = tile_img\n",
        "\n",
        "        # Optional: visualize the first few tiles\n",
        "        # plt.figure(figsize=(4, 2))\n",
        "        # plt.imshow(tile_img)\n",
        "        # plt.title(f\"Tile lat:{lat_i} lon:{lon_j}\")\n",
        "        # plt.axis(\"off\")\n",
        "        # plt.show()\n",
        "\n",
        "        # === Step 9: Combine tiles back into a single ERP-like image ===\n",
        "\n",
        "rows_combined = []\n",
        "for lat_i in range(num_lat_tiles):\n",
        "    # Stack all longitude tiles in this latitude row\n",
        "    row_tiles = finalTiles[lat_i]\n",
        "    row_img = np.hstack(row_tiles)\n",
        "    rows_combined.append(row_img)\n",
        "\n",
        "# Stack all latitude rows vertically (north → south)\n",
        "erp_reconstructed = np.vstack(rows_combined)\n",
        "\n",
        "print(\"Reconstructed ERP shape:\", erp_reconstructed.shape)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(erp_reconstructed)\n",
        "plt.title(\"Reconstructed ERP from 20°×20° rectified tiles\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.imshow(erp)\n",
        "plt.title(\"Step 1: Original ERP (Equirectangular Projection) Image\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "longNum = 41\n",
        "latNum = 4\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(finalTiles[latNum][longNum])\n",
        "plt.title(\"Reconstructed Tile\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "ogImage = np.array(erp)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(ogImage[latNum*tileHeight:latNum*tileHeight + tileHeight, longNum*tileWidth:longNum*tileWidth + tileWidth])\n",
        "plt.title(\"Original Tile\")\n",
        "plt.axis(\"off\")\n",
        "\n"
      ],
      "metadata": {
        "id": "irHUWPuQQ5Rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iteration 4"
      ],
      "metadata": {
        "id": "eB1mITFVRBQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 360° ERP Image → Rectified Tile Projection → Saliency Models\n",
        "# DeepLabV3 | U²-NetP | BASNet\n",
        "# ============================================================\n",
        "\n",
        "from IPython.display import Image as ColabImage\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import math\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms, models\n",
        "import os\n",
        "\n",
        "# ============================================================\n",
        "# STEP 1: Load ERP Image\n",
        "# ============================================================\n",
        "\n",
        "filename = \"Ocean360Picture.jpg\"\n",
        "erp = cv2.imread(filename)\n",
        "erp = cv2.cvtColor(erp, cv2.COLOR_BGR2RGB)\n",
        "H, W, _ = erp.shape\n",
        "print(f\"Height = {H}, Width = {W}\")\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(erp)\n",
        "plt.title(\"Original ERP (Equirectangular Projection) Image\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "# ============================================================\n",
        "# STEP 2: Rectified Tiling (20°×20°)\n",
        "# ============================================================\n",
        "\n",
        "widthDegree, heightDegree = 20, 20\n",
        "num_lon_tiles = int(360 / widthDegree)\n",
        "num_lat_tiles = int(180 / heightDegree)\n",
        "\n",
        "tileWidth = int(round(W / num_lon_tiles))\n",
        "tileHeight = int(round(H / num_lat_tiles))\n",
        "print(f\"Tile width = {tileWidth}, Tile height = {tileHeight}\")\n",
        "\n",
        "finalTiles = [[None for _ in range(num_lon_tiles)] for _ in range(num_lat_tiles)]\n",
        "\n",
        "for lat_i in range(num_lat_tiles):\n",
        "    lat_max = 90.0 - lat_i * heightDegree\n",
        "    lat_min = lat_max - heightDegree\n",
        "    lat_vals = np.linspace(lat_max, lat_min, tileHeight)\n",
        "\n",
        "    for lon_j in range(num_lon_tiles):\n",
        "        lon_min = lon_j * widthDegree - 180.0\n",
        "        lon_max = lon_min + widthDegree\n",
        "        lon_vals = np.linspace(lon_min, lon_max, tileWidth)\n",
        "\n",
        "        erp_x_vec = (lon_vals + 180.0) / 360.0 * (W - 1)\n",
        "        erp_y_vec = (90.0 - lat_vals) / 180.0 * (H - 1)\n",
        "\n",
        "        map_x = np.tile(erp_x_vec, (tileHeight, 1)).astype(np.float32)\n",
        "        map_y = np.repeat(erp_y_vec[:, np.newaxis], tileWidth, axis=1).astype(np.float32)\n",
        "\n",
        "        tile_img = cv2.remap(erp, map_x, map_y, interpolation=cv2.INTER_LINEAR, borderMode=cv2.BORDER_WRAP)\n",
        "        finalTiles[lat_i][lon_j] = tile_img\n",
        "\n",
        "# ============================================================\n",
        "# STEP 3: Reconstruct ERP from Rectified Tiles\n",
        "# ============================================================\n",
        "\n",
        "erp_reconstructed = np.vstack([np.hstack(row) for row in finalTiles])\n",
        "print(\"Reconstructed ERP shape:\", erp_reconstructed.shape)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(erp_reconstructed)\n",
        "plt.title(\"Reconstructed ERP from 20°×20° Rectified Tiles\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "# ============================================================\n",
        "# STEP 4: DeepLabV3\n",
        "# ============================================================\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(erp_reconstructed)\n",
        "plt.title(\"Reconstructed ERP from 20°×20° Rectified Tiles\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "deeplab = models.segmentation.deeplabv3_resnet50(pretrained=True).eval()\n",
        "transform_dl = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "saliencyTiles_dl = [[None for _ in range(num_lon_tiles)] for _ in range(num_lat_tiles)]\n",
        "\n",
        "for lat_i in range(num_lat_tiles):\n",
        "    for lon_j in range(num_lon_tiles):\n",
        "        tile_img = finalTiles[lat_i][lon_j]\n",
        "        if tile_img is None:\n",
        "            continue\n",
        "        input_tensor = transform_dl(tile_img).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            output = deeplab(input_tensor)[\"out\"][0]\n",
        "            saliency = torch.sigmoid(output[0]).cpu().numpy()\n",
        "        saliency_resized = cv2.resize(saliency, (tileWidth, tileHeight))\n",
        "        saliencyTiles_dl[lat_i][lon_j] = saliency_resized\n",
        "\n",
        "erp_sal_dl = np.vstack([np.hstack(row) for row in saliencyTiles_dl])\n",
        "erp_sal_dl = (erp_sal_dl - erp_sal_dl.min()) / (erp_sal_dl.max() - erp_sal_dl.min() + 1e-8)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(erp_sal_dl, cmap=\"inferno\")\n",
        "plt.title(\"Saliency Map (DeepLabV3 on Reconstructed ERP Tiles)\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "# ============================================================\n",
        "# STEP 5: U²-NetP\n",
        "# ============================================================\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(erp_reconstructed)\n",
        "plt.title(\"Reconstructed ERP from 20°×20° Rectified Tiles\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "!pip install -q gdown\n",
        "!git clone -q https://github.com/xuebinqin/U-2-Net.git\n",
        "%cd U-2-Net\n",
        "\n",
        "from model.u2net import U2NETP\n",
        "!gdown -q 1rbSTGKAE-MTxBYHd-51l2hMOQPT_7EPy -O u2netp.pth\n",
        "\n",
        "u2netp = U2NETP(3, 1)\n",
        "u2netp.load_state_dict(torch.load(\"u2netp.pth\", map_location=\"cpu\"))\n",
        "u2netp.eval()\n",
        "\n",
        "transform_u2 = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((320, 320)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "saliencyTiles_u2 = [[None for _ in range(num_lon_tiles)] for _ in range(num_lat_tiles)]\n",
        "\n",
        "for lat_i in range(num_lat_tiles):\n",
        "    for lon_j in range(num_lon_tiles):\n",
        "        tile_img = finalTiles[lat_i][lon_j]\n",
        "        if tile_img is None:\n",
        "            continue\n",
        "        input_tensor = transform_u2(tile_img).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            d1, *_ = u2netp(input_tensor)\n",
        "            pred = F.interpolate(d1, size=(tileHeight, tileWidth), mode=\"bilinear\", align_corners=False)\n",
        "            saliency = pred.squeeze().cpu().numpy()\n",
        "        saliency = (saliency - saliency.min()) / (saliency.max() - saliency.min() + 1e-8)\n",
        "        saliencyTiles_u2[lat_i][lon_j] = saliency\n",
        "\n",
        "erp_sal_u2 = np.vstack([np.hstack(row) for row in saliencyTiles_u2])\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(erp_sal_u2, cmap=\"inferno\")\n",
        "plt.title(\"Saliency Map (U²-NetP on Reconstructed ERP Tiles)\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# STEP 6: BASNet\n",
        "# ============================================================\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(erp_reconstructed)\n",
        "plt.title(\"Reconstructed ERP from 20°×20° Rectified Tiles\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "#!git clone -q https://github.com/xuebinqin/BASNet.git\n",
        "#from BASNet.model import BASNet\n",
        "#!gdown -q 1-Yg0cxgrNhHP-016FPdp902BR-kSsA4P -O BASNet.pth\n",
        "\n",
        "!curl -L -o BASNet.pth \"https://huggingface.co/creative-graphic-design/BASNet-checkpoints/resolve/main/basnet.pth\"\n",
        "\n",
        "from BASNet.model import BASNet\n",
        "import torch\n",
        "\n",
        "basnet = BASNet(3, 1)\n",
        "basnet.load_state_dict(torch.load(\"BASNet.pth\", map_location=\"cpu\"))\n",
        "basnet.eval()\n",
        "\n",
        "transform_bas = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((320, 320)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "saliencyTiles_bas = [[None for _ in range(num_lon_tiles)] for _ in range(num_lat_tiles)]\n",
        "\n",
        "for lat_i in range(num_lat_tiles):\n",
        "    for lon_j in range(num_lon_tiles):\n",
        "        tile_img = finalTiles[lat_i][lon_j]\n",
        "        if tile_img is None:\n",
        "            continue\n",
        "        input_tensor = transform_bas(tile_img).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            d1, *_ = basnet(input_tensor)\n",
        "            pred = F.interpolate(d1, size=(tileHeight, tileWidth), mode=\"bilinear\", align_corners=False)\n",
        "            saliency = pred.squeeze().cpu().numpy()\n",
        "        saliency = (saliency - saliency.min()) / (saliency.max() - saliency.min() + 1e-8)\n",
        "        saliencyTiles_bas[lat_i][lon_j] = saliency\n",
        "\n",
        "erp_sal_bas = np.vstack([np.hstack(row) for row in saliencyTiles_bas])\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(erp_sal_bas, cmap=\"inferno\")\n",
        "plt.title(\"Saliency Map (BASNet on Reconstructed ERP Tiles)\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7SWCaX4VRLY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iteration 5"
      ],
      "metadata": {
        "id": "pi6RUqNYRTkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 360° ERP Image → Rectified Tile Projection → Saliency Models\n",
        "# DeepLabV3 | U²-NetP | BASNet | MLNet | SalGAN\n",
        "# ============================================================\n",
        "\n",
        "from IPython.display import Image as ColabImage\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import math\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "import os\n",
        "\n",
        "# ============================================================\n",
        "# STEP 0: Setup\n",
        "# ============================================================\n",
        "\n",
        "!pip install -q gdown\n",
        "!git clone -q https://github.com/xuebinqin/U-2-Net.git\n",
        "%cd U-2-Net\n",
        "from model.u2net import U2NETP\n",
        "!gdown -q 1rbSTGKAE-MTxBYHd-51l2hMOQPT_7EPy -O u2netp.pth\n",
        "\n",
        "u2netp = U2NETP(3, 1)\n",
        "u2netp.load_state_dict(torch.load(\"u2netp.pth\", map_location=\"cpu\"))\n",
        "u2netp.eval()\n",
        "\n",
        "transform_u2 = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((320, 320)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# ============================================================\n",
        "# STEP 1–6: Define Processing Function\n",
        "# ============================================================\n",
        "\n",
        "def process_image(filename):\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"Processing: {filename}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # STEP 1: Load ERP Image\n",
        "    erp = cv2.imread(filename)\n",
        "    if erp is None:\n",
        "        print(f\"⚠️ File not found: {filename}\")\n",
        "        return\n",
        "    erp = cv2.cvtColor(erp, cv2.COLOR_BGR2RGB)\n",
        "    H, W, _ = erp.shape\n",
        "    print(f\"Height = {H}, Width = {W}\")\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(erp)\n",
        "    plt.title(f\"Original ERP: {os.path.basename(filename)}\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "    # STEP 2: Rectified Tiling (20°×20°)\n",
        "    widthDegree, heightDegree = 20, 20\n",
        "    num_lon_tiles = int(360 / widthDegree)\n",
        "    num_lat_tiles = int(180 / heightDegree)\n",
        "\n",
        "    tileWidth = int(round(W / num_lon_tiles))\n",
        "    tileHeight = int(round(H / num_lat_tiles))\n",
        "    print(f\"Tile width = {tileWidth}, Tile height = {tileHeight}\")\n",
        "\n",
        "    finalTiles = [[None for _ in range(num_lon_tiles)] for _ in range(num_lat_tiles)]\n",
        "\n",
        "    for lat_i in range(num_lat_tiles):\n",
        "        lat_max = 90.0 - lat_i * heightDegree\n",
        "        lat_min = lat_max - heightDegree\n",
        "        lat_vals = np.linspace(lat_max, lat_min, tileHeight)\n",
        "        for lon_j in range(num_lon_tiles):\n",
        "            lon_min = lon_j * widthDegree - 180.0\n",
        "            lon_max = lon_min + widthDegree\n",
        "            lon_vals = np.linspace(lon_min, lon_max, tileWidth)\n",
        "\n",
        "            erp_x_vec = (lon_vals + 180.0) / 360.0 * (W - 1)\n",
        "            erp_y_vec = (90.0 - lat_vals) / 180.0 * (H - 1)\n",
        "\n",
        "            map_x = np.tile(erp_x_vec, (tileHeight, 1)).astype(np.float32)\n",
        "            map_y = np.repeat(erp_y_vec[:, np.newaxis], tileWidth, axis=1).astype(np.float32)\n",
        "\n",
        "            tile_img = cv2.remap(erp, map_x, map_y, interpolation=cv2.INTER_LINEAR, borderMode=cv2.BORDER_WRAP)\n",
        "            finalTiles[lat_i][lon_j] = tile_img\n",
        "\n",
        "    # STEP 3: Reconstruct ERP from Rectified Tiles\n",
        "    erp_reconstructed = np.vstack([np.hstack(row) for row in finalTiles])\n",
        "    print(\"Reconstructed ERP shape:\", erp_reconstructed.shape)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.imshow(erp_reconstructed)\n",
        "    plt.title(\"Reconstructed ERP from 20°×20° Rectified Tiles\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "    # STEP 5: Run U²-NetP (Global)\n",
        "    input_full = transform_u2(erp_reconstructed).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        d1, *_ = u2netp(input_full)\n",
        "        pred_full = F.interpolate(d1, size=(H, W), mode=\"bilinear\", align_corners=False)\n",
        "        saliency_full = pred_full.squeeze().cpu().numpy()\n",
        "\n",
        "    saliency_full_resized = (saliency_full - saliency_full.min()) / (saliency_full.max() - saliency_full.min() + 1e-8)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.imshow(saliency_full_resized, cmap=\"inferno\")\n",
        "    plt.title(\"U²-NetP Saliency Map (Full ERP)\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "    # STEP 5b: Run U²-NetP (Tiles)\n",
        "    saliencyTiles_u2 = [[None for _ in range(num_lon_tiles)] for _ in range(num_lat_tiles)]\n",
        "\n",
        "    for lat_i in range(num_lat_tiles):\n",
        "        for lon_j in range(num_lon_tiles):\n",
        "            tile_img = finalTiles[lat_i][lon_j]\n",
        "            if tile_img is None:\n",
        "                continue\n",
        "            input_tensor = transform_u2(tile_img).unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                d1, *_ = u2netp(input_tensor)\n",
        "                pred = F.interpolate(d1, size=(tileHeight, tileWidth), mode=\"bilinear\", align_corners=False)\n",
        "                saliency = pred.squeeze().cpu().numpy()\n",
        "            saliency = (saliency - saliency.min()) / (saliency.max() - saliency.min() + 1e-8)\n",
        "            saliencyTiles_u2[lat_i][lon_j] = saliency\n",
        "\n",
        "    erp_sal_u2 = np.vstack([np.hstack(row) for row in saliencyTiles_u2])\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.imshow(erp_sal_u2, cmap=\"inferno\")\n",
        "    plt.title(\"Saliency Map (U²-NetP on Reconstructed ERP Tiles)\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "    # STEP 6: Percentage Difference\n",
        "    erp_sal_u2_resized = cv2.resize(erp_sal_u2, (W, H))\n",
        "    diff = np.abs(saliency_full_resized - erp_sal_u2_resized)\n",
        "    percent_diff = diff / (saliency_full_resized + 1e-8) * 100\n",
        "    mean_diff = np.mean(percent_diff)\n",
        "    print(f\"\\nAverage U²-NetP saliency difference between full ERP and tile-based ERP: {mean_diff:.2f}%\")\n",
        "\n",
        "    # STEP 7: Fusion\n",
        "    # Compute global scene complexity to adapt fusion weight (α)\n",
        "    gray_img = cv2.cvtColor(erp_reconstructed, cv2.COLOR_RGB2GRAY)\n",
        "    lap_var = cv2.Laplacian(gray_img, cv2.CV_64F).var()   # local detail measure\n",
        "    entropy = -np.sum(np.histogram(gray_img, bins=256, range=(0,255), density=True)[0] *\n",
        "                      np.log2(np.histogram(gray_img, bins=256, range=(0,255), density=True)[0] + 1e-8))\n",
        "\n",
        "    # Normalize both metrics to [0,1] scale\n",
        "    lap_var_norm = np.clip(lap_var / 500.0, 0, 1)\n",
        "    entropy_norm = np.clip(entropy / 8.0, 0, 1)\n",
        "\n",
        "    # Adaptive fusion weight (α)\n",
        "    # More detail → lower α (favor local)\n",
        "    # Less detail → higher α (favor global)\n",
        "    alpha = 0.3 + 0.7 * (1 - lap_var_norm * entropy_norm)\n",
        "\n",
        "    print(f\"Scene detail (Laplacian variance): {lap_var:.2f}, Entropy: {entropy:.2f}\")\n",
        "    print(f\"Adaptive fusion weight α = {alpha:.2f}  (higher = more global, lower = more local)\")\n",
        "\n",
        "    erp_sal_u2_resized = cv2.resize(erp_sal_u2, (W, H))\n",
        "    saliency_fused = alpha * saliency_full_resized + (1 - alpha) * erp_sal_u2_resized\n",
        "    saliency_fused = cv2.GaussianBlur(saliency_fused, (5, 5), 1)\n",
        "    saliency_fused = (saliency_fused - saliency_fused.min()) / (saliency_fused.max() - saliency_fused.min() + 1e-8)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.imshow(saliency_fused, cmap=\"inferno\")\n",
        "    plt.title(f\"Adaptive Fused Saliency Map (α={alpha:.2f})\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "    # Compare fused vs. global\n",
        "    diff_fused = np.abs(saliency_full_resized - saliency_fused)\n",
        "    relative_diff = diff_fused / ((saliency_full_resized + saliency_fused) / 2 + 1e-8) * 100\n",
        "    mean_diff_fused = np.mean(relative_diff)\n",
        "    print(f\"Average relative difference between fused and global saliency: {mean_diff_fused:.2f}%\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# ============================================================\n",
        "# STEP 8: Run for All Files\n",
        "# ============================================================\n",
        "\n",
        "file_list = [\n",
        "    \"/content/Ocean360Picture.jpg\",\n",
        "    \"/content/aerial-drone-panorama-view-village-located-near-river-hills-fields-godrays-clouds-moldova.jpg\",\n",
        "    \"/content/new-york-city-manhattan.jpg\"\n",
        "]\n",
        "\n",
        "for f in file_list:\n",
        "    process_image(f)"
      ],
      "metadata": {
        "id": "WqiQ9FmMRTTj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}