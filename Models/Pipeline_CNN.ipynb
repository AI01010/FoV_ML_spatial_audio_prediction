{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Create 4D heatmaps and store from 360 ERP + 7 audio heatmaps"
      ],
      "metadata": {
        "id": "-BKZB7O725ld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "from scipy.integrate import dblquad\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "def compute_audio_at_direction(W: np.ndarray, X: np.ndarray,\n",
        "                               Y: np.ndarray, Z: np.ndarray,\n",
        "                               top_left: tuple, bottom_right: tuple,\n",
        "                               center_time: float, sampleRate: int,\n",
        "                               window_sec: float = 0.1) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Compute audio waveform for a tile and around a specific time.\n",
        "\n",
        "    Parameters:\n",
        "        center_time: time in seconds to center the window\n",
        "        window_sec: half-length of window (seconds) to extract\n",
        "    \"\"\"\n",
        "    top_left_lat, top_left_lon = top_left\n",
        "    bottom_right_lat, bottom_right_lon = bottom_right\n",
        "\n",
        "    # Convert bounds to radians\n",
        "    lat_min_rad = np.radians(bottom_right_lat)\n",
        "    lat_max_rad = np.radians(top_left_lat)\n",
        "    lon_min_rad = np.radians(top_left_lon)\n",
        "    lon_max_rad = np.radians(bottom_right_lon)\n",
        "\n",
        "    # Compute patch area (steradians)\n",
        "    area = (lon_max_rad - lon_min_rad) * (np.sin(lat_max_rad) - np.sin(lat_min_rad))\n",
        "\n",
        "    # Integrate Y_W over the region\n",
        "    integral_Y_W, _ = dblquad(lambda lon, lat: np.cos(lat), lat_min_rad, lat_max_rad,\n",
        "                              lambda _: lon_min_rad, lambda _: lon_max_rad)\n",
        "    integral_Y_X, _ = dblquad(lambda lon, lat: np.cos(lat)*np.cos(lon)*np.cos(lat), lat_min_rad, lat_max_rad,\n",
        "                              lambda _: lon_min_rad, lambda _: lon_max_rad)\n",
        "    integral_Y_Y, _ = dblquad(lambda lon, lat: np.cos(lat)*np.sin(lon)*np.cos(lat), lat_min_rad, lat_max_rad,\n",
        "                              lambda _: lon_min_rad, lambda _: lon_max_rad)\n",
        "    integral_Y_Z, _ = dblquad(lambda lon, lat: np.sin(lat)*np.cos(lat), lat_min_rad, lat_max_rad,\n",
        "                              lambda _: lon_min_rad, lambda _: lon_max_rad)\n",
        "\n",
        "    # Convert time to sample indices\n",
        "    center_sample = int(center_time * sampleRate)\n",
        "    half_window_samples = int(window_sec * sampleRate)\n",
        "    start = max(center_sample - half_window_samples, 0)\n",
        "    end = min(center_sample + half_window_samples, len(W))\n",
        "\n",
        "    # Extract the waveform slice\n",
        "    W_slice = W[start:end]\n",
        "    X_slice = X[start:end]\n",
        "    Y_slice = Y[start:end]\n",
        "    Z_slice = Z[start:end]\n",
        "\n",
        "    # Reconstruct waveform for this tile\n",
        "    wave = (integral_Y_W * W_slice + integral_Y_X * X_slice +\n",
        "            integral_Y_Y * Y_slice + integral_Y_Z * Z_slice) / area\n",
        "\n",
        "    return wave\n",
        "\n",
        "def computeHNR(frame):\n",
        "    \"\"\"\n",
        "    Compute HNR for a single frame using autocorrelation.\n",
        "    HNR = 10 * log10(energy_harmonic / energy_noise)\n",
        "    \"\"\"\n",
        "    frame = frame - np.mean(frame)  # remove DC\n",
        "    if np.all(frame == 0):\n",
        "        return 0.0\n",
        "\n",
        "    # FFT-based autocorrelation\n",
        "    autocorr = np.fft.irfft(np.fft.rfft(frame) * np.conj(np.fft.rfft(frame)))\n",
        "    autocorr = autocorr / np.max(np.abs(autocorr))  # normalize\n",
        "\n",
        "    # Harmonic energy = max autocorr (excluding lag 0)\n",
        "    harmonic_energy = np.max(autocorr[1:])\n",
        "    # Noise energy = lag 0 minus harmonic energy\n",
        "    noise_energy = autocorr[0] - harmonic_energy\n",
        "    if noise_energy <= 0:\n",
        "        return 40.0  # cap to a reasonable max\n",
        "    return 10 * np.log10(harmonic_energy / noise_energy)\n",
        "\n",
        "def processWave(wave, sampleRate):\n",
        "    windowSize = 2048\n",
        "    hopSize = 100\n",
        "\n",
        "    # converts this to a Short-Time Fourier Transform. Tells you how much eergy has at each frequency over time.\n",
        "    # does this by going through windows. Length of each window defined by n_fft. Then, shifts window to right by length\n",
        "    # hop length. at each window, computes how much of each frequency is present.\n",
        "    # final value is 2D array of rows being each frequency, columns being time (which is now the windows), so value being amplitude/energy for that time and frequency\n",
        "    stftWave = np.abs(librosa.stft(wave, n_fft=windowSize, hop_length=hopSize))\n",
        "    # when we get the mel, that just converts all the frequencies to 128 possible onces, which are moreso frequencies humans can hear. So compressing\n",
        "    # the frequencies from a large number of frequencies to a smaller number, in this case n_mels amount\n",
        "    mel = librosa.feature.melspectrogram(S = stftWave, sr= sampleRate, n_mels = 128)\n",
        "    # converts from power scaling of audio to decibel scaling, cause humans perceive in moreso logarithm of audio (so higher sounds kinda taper off to us)\n",
        "    logMel = librosa.power_to_db(mel, ref=np.max)\n",
        "\n",
        "    # gets overall frame energy, including amplitude\n",
        "    volumeNorm = np.mean((logMel + 80), axis=0)\n",
        "    # gets the contrast in energy between frequencies within a specific frequency band, so where some frequencies bands may have parts of high energy frequencies, while other parts are low energy\n",
        "    contrast = librosa.feature.spectral_contrast(S = stftWave, sr=sampleRate)\n",
        "    # combines the difference frequency bands to get a average contrast for that time frame\n",
        "    contrast = np.mean(contrast, axis=0)\n",
        "    # basically gets how much the sound chagnes over time. Does this by getting differnece over time fimes with np.diff, squaring that value, and getting its sum\n",
        "    temporal_novelty = np.sum(np.diff(logMel, axis=1) ** 2, axis=0)\n",
        "    # do this to add an extra value cause rn, the length is T - 1, since you're getting difference between frames. So add 1 to get it to T length\n",
        "    temporal_novelty = np.insert(temporal_novelty, 0, 0)\n",
        "\n",
        "    # this gets how noise like a sound is, whether it's tonal or liek white noise. A tonal sound is one that just\n",
        "    # stands out, like through sharp peaks.\n",
        "    spectral_flatness = librosa.feature.spectral_flatness(y=wave, n_fft=windowSize, hop_length= hopSize)\n",
        "    # gets the average frequency weighted by amplitude, how \"bright\" the sound is, sees if the audio tends to have more high frequency or low frequency sounds\n",
        "    centroid = librosa.feature.spectral_centroid(S=stftWave, sr=sampleRate)\n",
        "    # indicates the range of frequencies present, so if the frequencies are more concentrated or spread out\n",
        "    bandwidth = librosa.feature.spectral_bandwidth(S=stftWave, sr=sampleRate)\n",
        "\n",
        "\n",
        "\n",
        "    # Compute HNR per frame. HNR is how harmonic the sound is, if its harmonic, with a pattern, or more noisy.\n",
        "    # different from spectral flatness in that it measures if its harmonic, as opposed to tonal. Basically if there's like\n",
        "    # a repeating pattern that identifies the town\n",
        "    hnr_values = []\n",
        "    num_frames = stftWave.shape[1]\n",
        "    for i in range(num_frames):\n",
        "        frame = wave[i*hopSize : i*hopSize + windowSize]\n",
        "        if len(frame) < 2:\n",
        "            continue\n",
        "        hnr_values.append(computeHNR(frame))\n",
        "    hnr_values = np.array(hnr_values)\n",
        "\n",
        "    return np.array([np.mean(volumeNorm), np.mean(contrast), np.mean(temporal_novelty), np.mean(hnr_values), np.mean(spectral_flatness), np.mean(centroid), np.mean(bandwidth)])\n",
        "\n",
        "def precompute_integrals(tile_size_deg=20):\n",
        "    \"\"\"\n",
        "    Precompute integrals for coarse tiles (done once, cached for all frames).\n",
        "\n",
        "    Parameters:\n",
        "        tile_size_deg: size of each tile in degrees (default 20x20)\n",
        "\n",
        "    Returns:\n",
        "        Dictionary mapping (lat, lon) tile coordinates to integral values and area\n",
        "    \"\"\"\n",
        "    print(f\"Precomputing integrals for {tile_size_deg}° tiles...\")\n",
        "\n",
        "    tile_cache = {}\n",
        "\n",
        "    # Generate all tile coordinates\n",
        "    latitudes = list(range(90, -90, -tile_size_deg))\n",
        "    longitudes = list(range(-180, 180, tile_size_deg))\n",
        "\n",
        "    for topLeftLat in latitudes:\n",
        "        for topLeftLon in longitudes:\n",
        "            bottom_right_lat, bottom_right_lon = (topLeftLat - tile_size_deg, topLeftLon + tile_size_deg)\n",
        "\n",
        "            # Convert bounds to radians\n",
        "            lat_min_rad = np.radians(bottom_right_lat)\n",
        "            lat_max_rad = np.radians(topLeftLat)\n",
        "            lon_min_rad = np.radians(topLeftLon)\n",
        "            lon_max_rad = np.radians(bottom_right_lon)\n",
        "\n",
        "            # Compute patch area\n",
        "            area = (lon_max_rad - lon_min_rad) * (np.sin(lat_max_rad) - np.sin(lat_min_rad))\n",
        "\n",
        "            # Compute integrals\n",
        "            integral_Y_W, _ = dblquad(\n",
        "                lambda lon, lat: np.cos(lat), lat_min_rad, lat_max_rad,\n",
        "                lambda _: lon_min_rad, lambda _: lon_max_rad)\n",
        "            integral_Y_X, _ = dblquad(\n",
        "                lambda lon, lat: np.cos(lat)*np.cos(lon)*np.cos(lat), lat_min_rad, lat_max_rad,\n",
        "                lambda _: lon_min_rad, lambda _: lon_max_rad)\n",
        "            integral_Y_Y, _ = dblquad(\n",
        "                lambda lon, lat: np.cos(lat)*np.sin(lon)*np.cos(lat), lat_min_rad, lat_max_rad,\n",
        "                lambda _: lon_min_rad, lambda _: lon_max_rad)\n",
        "            integral_Y_Z, _ = dblquad(\n",
        "                lambda lon, lat: np.sin(lat)*np.cos(lat), lat_min_rad, lat_max_rad,\n",
        "                lambda _: lon_min_rad, lambda _: lon_max_rad)\n",
        "\n",
        "            tile_cache[(topLeftLat, topLeftLon)] = (\n",
        "                area,\n",
        "                integral_Y_W,\n",
        "                integral_Y_X,\n",
        "                integral_Y_Y,\n",
        "                integral_Y_Z\n",
        "            )\n",
        "\n",
        "    print(\"Integral precomputation complete!\")\n",
        "    return tile_cache\n",
        "\n",
        "\n",
        "def get_tile_for_pixel(lat_pixel, lon_pixel, erp_height, erp_width, tile_size_deg=20):\n",
        "    \"\"\"\n",
        "    Get the tile coordinates for a given pixel in the ERP map.\n",
        "    \"\"\"\n",
        "    # Convert pixel to lat/lon\n",
        "    lat = 90 - (lat_pixel / erp_height) * 180\n",
        "    lon = -180 + (lon_pixel / erp_width) * 360\n",
        "\n",
        "    # Find which tile this belongs to\n",
        "    tile_lat = int(np.floor(lat / tile_size_deg)) * tile_size_deg\n",
        "    tile_lon = int(np.floor(lon / tile_size_deg)) * tile_size_deg\n",
        "\n",
        "    return (tile_lat, tile_lon)\n",
        "\n",
        "\n",
        "def compute_audio_saliency_heatmap_vectorized(W, X, Y, Z, audio_samplerate,\n",
        "                                              frame_idx, video_fps,\n",
        "                                              erp_height, erp_width,\n",
        "                                              tile_cache,  sample_every_n_frames,\n",
        "                                              numHeatmaps, tile_size_deg=20):\n",
        "    \"\"\"\n",
        "    Compute audio saliency heatmap for a given frame using precomputed tile integrals.\n",
        "    Each tile's saliency is computed once and replicated to all pixels in that tile.\n",
        "    Extracts audio from 2.5 frames before to 2.5 frames after the current frame.\n",
        "    Returns a 2D array of shape (erp_height, erp_width).\n",
        "    \"\"\"\n",
        "    # Time in seconds corresponding to this video frame\n",
        "    time_sec = frame_idx / video_fps\n",
        "\n",
        "    # Window: 2.5 frames before and 2.5 frames after = 5 frames total\n",
        "    frameWindow = sample_every_n_frames / 2\n",
        "    window_sec = frameWindow / video_fps\n",
        "\n",
        "    # Convert time to sample indices\n",
        "    center_sample = int(time_sec * audio_samplerate)\n",
        "    half_window_samples = int(window_sec * audio_samplerate)\n",
        "    start = max(center_sample - half_window_samples, 0)\n",
        "    end = min(center_sample + half_window_samples, len(W))\n",
        "\n",
        "    # Extract waveform slices\n",
        "    W_slice = W[start:end]\n",
        "    X_slice = X[start:end]\n",
        "    Y_slice = Y[start:end]\n",
        "    Z_slice = Z[start:end]\n",
        "\n",
        "    # Initialize output saliency map\n",
        "    saliency_map = np.zeros((numHeatmaps, erp_height, erp_width))\n",
        "\n",
        "    numLatTiles = 180 // tile_size_deg\n",
        "    numLonTiles = 360 // tile_size_deg\n",
        "\n",
        "    # Calculate pixels per tile\n",
        "    pixels_per_tile_lat = erp_height // numLatTiles  # 9 tiles in latitude\n",
        "    pixels_per_tile_lon = erp_width // numLonTiles   # 18 tiles in longitude\n",
        "\n",
        "    # Iterate through each tile\n",
        "    for lat_tile in range(numLatTiles):\n",
        "        for lon_tile in range(numLonTiles):\n",
        "            # Get tile coordinates\n",
        "            tile_lat = 90 - lat_tile * tile_size_deg\n",
        "            tile_lon = -180 + lon_tile * tile_size_deg\n",
        "            tile_coords = (tile_lat, tile_lon)\n",
        "\n",
        "            if tile_coords in tile_cache:\n",
        "                area, integral_Y_W, integral_Y_X, integral_Y_Y, integral_Y_Z = tile_cache[tile_coords]\n",
        "\n",
        "                # Reconstruct waveform for this tile\n",
        "                wave = (integral_Y_W * W_slice +\n",
        "                       integral_Y_X * X_slice +\n",
        "                       integral_Y_Y * Y_slice +\n",
        "                       integral_Y_Z * Z_slice) / area\n",
        "\n",
        "                if len(wave) > 0:\n",
        "                    saliency_values = processWave(wave, audio_samplerate)\n",
        "                else:\n",
        "                    saliency_values = np.zeros(numHeatmaps)\n",
        "\n",
        "                # Fill all pixels in this tile with the same saliency value\n",
        "                y_start = lat_tile * pixels_per_tile_lat\n",
        "                y_end = (lat_tile + 1) * pixels_per_tile_lat\n",
        "                x_start = lon_tile * pixels_per_tile_lon\n",
        "                x_end = (lon_tile + 1) * pixels_per_tile_lon\n",
        "\n",
        "                # weirdly works cause of python's funky mapping\n",
        "                saliency_map[:, y_start:y_end, x_start:x_end] = saliency_values[:, np.newaxis, np.newaxis]\n",
        "            else:\n",
        "                raise IndexError(\"IDk how we got this.\")\n",
        "\n",
        "    return saliency_map\n",
        "\n",
        "\n",
        "def compute_video_saliency_heatmap_vectorized(frame_idx, video_fps,\n",
        "                                              erp_height, erp_width,\n",
        "                                              tile_cache,  sample_every_n_frames,\n",
        "                                              numHeatmaps, tile_size_deg=20):\n",
        "    \"\"\"\n",
        "    Compute video saliency heatmap for a given frame using ERP tile.\n",
        "    Saliency is computed once on full ERP per frame.\n",
        "    For motion/optical flow saliency use the current and previous frame (if exists)\n",
        "    Returns a 2D array of shape (erp_height, erp_width).\n",
        "    \"\"\"\n",
        "    # Time in seconds corresponding to this video frame\n",
        "    time_sec = frame_idx / video_fps\n",
        "\n",
        "    # Initialize output saliency map\n",
        "    saliency_map = np.zeros((numHeatmaps, erp_height, erp_width))\n",
        "\n",
        "    numLatTiles = 180 // tile_size_deg\n",
        "    numLonTiles = 360 // tile_size_deg\n",
        "\n",
        "    # Calculate pixels per tile\n",
        "    pixels_per_tile_lat = erp_height // numLatTiles  # 9 tiles in latitude\n",
        "    pixels_per_tile_lon = erp_width // numLonTiles   # 18 tiles in longitude\n",
        "\n",
        "    # Example usage SalNet:\n",
        "    # SalientStaticImage = process_image(Whereever image variable is)                    --- Image -> ERP Image -> ERP HeatMap\n",
        "    # SalientStaticImageToTiles = saliency_to_rectified_tiles(saliency_full_resized)     --- ERP HeatMap -> ERP HeatMap Tiles\n",
        "\n",
        "    # Example usage FlowNet:\n",
        "    # img1 = cv2.cvtColor(cv2.imread(\"/content/frame1.jpg\"), cv2.COLOR_BGR2RGB)          --- Frame 1\n",
        "    # img2 = cv2.cvtColor(cv2.imread(\"/content/frame2.jpg\"), cv2.COLOR_BGR2RGB)          --- Frame 2\n",
        "    # flow = compute_flow(img1, img2, model)                                             --- Flow between Frame 1 & 2\n",
        "    # heatmap_img = generate_flow_heatmap(flow)                                          --- HeatMap from Flow\n",
        "\n",
        "    return saliency_map\n",
        "\n",
        "\n",
        "def normalize_heatmaps(heatmaps):\n",
        "    \"\"\"Normalize heatmap to [0, 1] range.\"\"\"\n",
        "    # returns a list of mins and maxs for each heatmap\n",
        "    h_mins = np.min(heatmaps, axis=(1, 2), keepdims=True)\n",
        "    h_maxs = np.max(heatmaps, axis=(1, 2), keepdims=True)\n",
        "\n",
        "    return (heatmaps - h_mins) / (h_maxs - h_mins)\n",
        "\n",
        "\n",
        "def getFrame(cap, output_width, output_height, frame_idx):\n",
        "    \"\"\"\n",
        "    Read video and yield resized frames.\n",
        "    \"\"\"\n",
        "\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
        "    ret, frame = cap.read()\n",
        "    resized_frame = cv2.resize(frame, (output_width, output_height), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "    return resized_frame\n",
        "\n",
        "def process_360_video(video_path, audio_path, output_path,\n",
        "                      erp_height=1920, erp_width=3840, sample_every_n_frames=5, numHeatmaps=9):\n",
        "    \"\"\"\n",
        "    Main pipeline to process a 360 video and extract audio saliency heatmaps.\n",
        "\n",
        "    Parameters:\n",
        "        video_path: path to ERP format 360 video\n",
        "        audio_path: path to first-order ambisonic audio file\n",
        "        output_path: where to save the output .npy file\n",
        "        erp_height: height of ERP format (pixels)\n",
        "        erp_width: width of ERP format (pixels)\n",
        "        sample_every_n_frames: sample every N frames\n",
        "    \"\"\"\n",
        "\n",
        "    # Load audio\n",
        "    print(\"Loading ambisonic audio...\")\n",
        "    audio_data, audio_samplerate = sf.read(audio_path)\n",
        "\n",
        "    # Check for 4 channels\n",
        "    if len(audio_data.shape) == 1:\n",
        "        raise ValueError(f\"Audio is mono. Expected 4-channel first-order ambisonics.\")\n",
        "    elif audio_data.shape[1] != 4:\n",
        "        raise ValueError(f\"Audio has {audio_data.shape[1]} channels. Expected 4-channel first-order ambisonics (W, X, Y, Z).\")\n",
        "\n",
        "    # Split into channels\n",
        "    W = audio_data[:, 0]\n",
        "    X = audio_data[:, 1]\n",
        "    Y = audio_data[:, 2]\n",
        "    Z = audio_data[:, 3]\n",
        "\n",
        "    print(f\"Audio shape: {audio_data.shape}\")\n",
        "    print(f\"Audio sample rate: {audio_samplerate} Hz\")\n",
        "    print(\"Successfully loaded 4-channel first-order ambisonics audio\")\n",
        "\n",
        "    # Open video to get metadata\n",
        "    print(\"Opening video...\")\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        raise ValueError(f\"Could not open video: {video_path}\")\n",
        "\n",
        "    video_fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    video_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    video_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    print(f\"Video FPS: {video_fps}\")\n",
        "    print(f\"Total frames: {total_frames}\")\n",
        "    print(f\"Video dimensions: {video_width}x{video_height}\")\n",
        "    print(f\"Target ERP dimensions: {erp_width}x{erp_height}\")\n",
        "\n",
        "    # Check if resizing is needed\n",
        "    need_resize = video_width != erp_width or video_height != erp_height\n",
        "    if need_resize:\n",
        "        print(f\"Video will be resized from {video_width}x{video_height} to {erp_width}x{erp_height}\")\n",
        "\n",
        "    # Precompute integrals for coarse tiles (20x20 degrees)\n",
        "    tile_cache = precompute_integrals(tile_size_deg=20)\n",
        "\n",
        "    # Calculate number of sampled frames\n",
        "    num_sampled_frames = (total_frames - math.ceil(sample_every_n_frames / 2)) // sample_every_n_frames\n",
        "\n",
        "    # Initialize output array\n",
        "    output_array = np.zeros((num_sampled_frames, numHeatmaps, erp_height, erp_width), dtype=np.float16)\n",
        "\n",
        "    print(f\"Output array shape: {output_array.shape}\")\n",
        "    print(f\"Processing {num_sampled_frames} frames...\")\n",
        "\n",
        "    # Use frame generator (resizes all frames upfront in the stream). Also, only retrieves them one at a time, instead of keeping it all in memory\n",
        "    for sampled_frame_idx in range(num_sampled_frames):\n",
        "        frame_idx = sample_every_n_frames * (sampled_frame_idx + 1)\n",
        "\n",
        "        frame = getFrame(cap, erp_height, erp_width, frame_idx)\n",
        "\n",
        "        print(f\"Processing frame {frame_idx}/{total_frames} (sample {sampled_frame_idx}/{num_sampled_frames})\")\n",
        "\n",
        "\n",
        "        # Compute audio-visual saliency heatmap\n",
        "        saliency_heatmaps = np.concat(compute_audio_saliency_heatmap_vectorized(W, X, Y, Z, audio_samplerate,\n",
        "                                                                        frame_idx, video_fps,\n",
        "                                                                        erp_height, erp_width,\n",
        "                                                                        tile_cache, sample_every_n_frames,\n",
        "                                                                        numHeatmaps-2, tile_size_deg=20),\n",
        "                                      compute_video_saliency_heatmap_vectorized(frame_idx, video_fps,\n",
        "                                                                        erp_height, erp_width,\n",
        "                                                                        tile_cache, sample_every_n_frames,\n",
        "                                                                        numHeatmaps-7, tile_size_deg=20)\n",
        "                                      )\n",
        "\n",
        "        # Normalize heatmap\n",
        "        saliency_heatmaps = normalize_heatmaps(saliency_heatmaps)\n",
        "\n",
        "        # Store in output array\n",
        "        output_array[sampled_frame_idx] = saliency_heatmaps\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    # Save output\n",
        "    print(f\"Saving output to {output_path}...\")\n",
        "    np.save(output_path, output_array)\n",
        "\n",
        "    print(f\"Done! Output shape: {output_array.shape}\")\n",
        "    print(f\"Saved to: {output_path}\")\n",
        "\n",
        "    return output_array\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    os.chdir(\"./../..\")\n",
        "\n",
        "    # Configuration - modify as needed\n",
        "    ERP_WIDTH = 3840  # width\n",
        "    ERP_HEIGHT = 1920  # height\n",
        "    SAMPLE_RATE = 5  # sample every 5 frames\n",
        "    FILE_NAME = \"5020\"\n",
        "    VIDEO_PATH = f\"Data/Pre-Processed-Data/{FILE_NAME}.mp4\"  # ERP format 360 video\n",
        "    AUDIO_PATH = f\"Data/Pre-Processed-Data/{FILE_NAME}.wav\"\n",
        "    OUTPUT_PATH = f\"FinalTrainingData/{FILE_NAME}.npy\"\n",
        "    numHeatmaps = 9\n",
        "\n",
        "    # Run the pipeline\n",
        "    saliency_array = process_360_video(VIDEO_PATH, AUDIO_PATH, OUTPUT_PATH,\n",
        "                                      erp_height=1920, erp_width=3840,\n",
        "                                      sample_every_n_frames=5, numHeatmaps=9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "ykN7TF2cycm7",
        "outputId": "addf6fa8-3759-4309-9100-de3eb58c3e4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading ambisonic audio...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "LibsndfileError",
          "evalue": "Error opening 'Data/Pre-Processed-Data/5020.wav': System error.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLibsndfileError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3589368408.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Run the pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     saliency_array = process_360_video(VIDEO_PATH, AUDIO_PATH, OUTPUT_PATH,\n\u001b[0m\u001b[1;32m    450\u001b[0m                                       \u001b[0merp_height\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1920\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merp_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3840\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                                       sample_every_n_frames=5, numHeatmaps=9)\n",
            "\u001b[0;32m/tmp/ipython-input-3589368408.py\u001b[0m in \u001b[0;36mprocess_360_video\u001b[0;34m(video_path, audio_path, output_path, erp_height, erp_width, sample_every_n_frames, numHeatmaps)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0;31m# Load audio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading ambisonic audio...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m     \u001b[0maudio_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_samplerate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0;31m# Check for 4 channels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/soundfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(file, frames, start, stop, dtype, always_2d, fill_value, out, samplerate, channels, format, subtype, endian, closefd)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \"\"\"\n\u001b[0;32m--> 305\u001b[0;31m     with SoundFile(file, 'r', samplerate, channels,\n\u001b[0m\u001b[1;32m    306\u001b[0m                    subtype, endian, format, closefd) as f:\n\u001b[1;32m    307\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/soundfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd, compression_level, bitrate_mode)\u001b[0m\n\u001b[1;32m    688\u001b[0m         self._info = _create_info_struct(file, mode, samplerate, channels,\n\u001b[1;32m    689\u001b[0m                                          format, subtype, endian)\n\u001b[0;32m--> 690\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosefd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missuperset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'r+'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseekable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0;31m# Move write position to 0 (like in Python file objects)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/soundfile.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[1;32m   1263\u001b[0m             \u001b[0;31m# get the actual error code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m             \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_snd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msf_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1265\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mLibsndfileError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Error opening {0!r}: \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1266\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode_int\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_snd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSFM_WRITE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m             \u001b[0;31m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLibsndfileError\u001b[0m: Error opening 'Data/Pre-Processed-Data/5020.wav': System error."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video Static + Motion/Optical Flow Saliency"
      ],
      "metadata": {
        "id": "4_T96j2uHi0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image as ColabImage\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import math\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "import os\n",
        "\n",
        "# ============================================================\n",
        "# STEP 0: Setup\n",
        "# ============================================================\n",
        "\n",
        "MODEL_PATH = \"/content/U-2-Net/u2netp.pth\"\n",
        "URL_DL = \"https://drive.google.com/uc?export=download&id=1rbSTGKAE-MTxBYHd-51l2hMOQPT_7EPy\"\n",
        "\n",
        "# Ensure folder exists\n",
        "os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)\n",
        "\n",
        "# Download if missing\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "    print(\"⬇ Downloading U2NetP weights…\")\n",
        "    result = os.system(f\"wget -q --no-check-certificate '{URL_DL}' -O {MODEL_PATH}\")\n",
        "    if result != 0 or not os.path.exists(MODEL_PATH):\n",
        "        raise FileNotFoundError(\"❌ Failed to download U2NetP.\")\n",
        "    print(\"✔ Download succeeded! Saved to:\", MODEL_PATH)\n",
        "else:\n",
        "    print(\"✔ Using cached u2netp.pth\")\n",
        "\n",
        "\n",
        "!git clone https://github.com/xuebinqin/U-2-Net.git /content/U-2-Net-Repo\n",
        "!pip install -q torch torchvision\n",
        "import sys\n",
        "sys.path.append('/content/U-2-Net-Repo')\n",
        "from model.u2net import U2NETP\n",
        "\n",
        "\n",
        "# Then load model as before\n",
        "u2netp = U2NETP(3, 1)\n",
        "u2netp.load_state_dict(torch.load(MODEL_PATH, map_location=\"cpu\"))\n",
        "u2netp.eval()\n",
        "print(\"✔ U2 Net Model ready!\")\n",
        "\n",
        "transform_u2 = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((320, 320)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# ============================================================\n",
        "# STEP 1–4: Define Processing Function\n",
        "# ============================================================\n",
        "\n",
        "def process_image(filename):\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"Processing: {filename}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # STEP 1: Load ERP Image\n",
        "    erp = cv2.imread(filename)\n",
        "    if erp is None:\n",
        "        print(f\"⚠️ File not found: {filename}\")\n",
        "        return\n",
        "    erp = cv2.cvtColor(erp, cv2.COLOR_BGR2RGB)\n",
        "    H, W, _ = erp.shape\n",
        "    print(f\"Height = {H}, Width = {W}\")\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(erp)\n",
        "    plt.title(f\"Original ERP: {os.path.basename(filename)}\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "    # STEP 2: Rectified Tiling (20°×20°)\n",
        "    widthDegree, heightDegree = 20, 20\n",
        "    num_lon_tiles = int(360 / widthDegree)\n",
        "    num_lat_tiles = int(180 / heightDegree)\n",
        "\n",
        "    tileWidth = int(round(W / num_lon_tiles))\n",
        "    tileHeight = int(round(H / num_lat_tiles))\n",
        "    print(f\"Tile width = {tileWidth}, Tile height = {tileHeight}\")\n",
        "\n",
        "    finalTiles = [[None for _ in range(num_lon_tiles)] for _ in range(num_lat_tiles)]\n",
        "\n",
        "    for lat_i in range(num_lat_tiles):\n",
        "        lat_max = 90.0 - lat_i * heightDegree\n",
        "        lat_min = lat_max - heightDegree\n",
        "        lat_vals = np.linspace(lat_max, lat_min, tileHeight)\n",
        "\n",
        "        for lon_j in range(num_lon_tiles):\n",
        "            lon_min = lon_j * widthDegree - 180.0\n",
        "            lon_max = lon_min + widthDegree\n",
        "            lon_vals = np.linspace(lon_min, lon_max, tileWidth)\n",
        "\n",
        "            erp_x_vec = (lon_vals + 180.0) / 360.0 * (W - 1)\n",
        "            erp_y_vec = (90.0 - lat_vals) / 180.0 * (H - 1)\n",
        "\n",
        "            map_x = np.tile(erp_x_vec, (tileHeight, 1)).astype(np.float32)\n",
        "            map_y = np.repeat(erp_y_vec[:, np.newaxis], tileWidth, axis=1).astype(np.float32)\n",
        "\n",
        "            tile_img = cv2.remap(erp, map_x, map_y, interpolation=cv2.INTER_LINEAR, borderMode=cv2.BORDER_WRAP)\n",
        "            finalTiles[lat_i][lon_j] = tile_img\n",
        "\n",
        "    # STEP 3: Reconstruct ERP from Rectified Tiles\n",
        "    erp_reconstructed = np.vstack([np.hstack(row) for row in finalTiles])\n",
        "    #print(\"Reconstructed ERP shape:\", erp_reconstructed.shape)\n",
        "\n",
        "    #plt.figure(figsize=(12, 6))\n",
        "    #plt.imshow(erp_reconstructed)\n",
        "    #plt.title(\"Reconstructed ERP from 20°×20° Rectified Tiles\")\n",
        "    #plt.axis(\"off\")\n",
        "    #plt.show()\n",
        "\n",
        "    # STEP 4: Run U²-NetP (Global)\n",
        "    input_full = transform_u2(erp_reconstructed).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        d1, *_ = u2netp(input_full)\n",
        "        pred_full = F.interpolate(d1, size=(H, W), mode=\"bilinear\", align_corners=False)\n",
        "        saliency_full = pred_full.squeeze().cpu().numpy()\n",
        "\n",
        "    saliency_full_resized = (saliency_full - saliency_full.min()) / (saliency_full.max() - saliency_full.min() + 1e-8)\n",
        "\n",
        "    #plt.figure(figsize=(12, 6))\n",
        "    #plt.imshow(saliency_full_resized, cmap=\"inferno\")\n",
        "    #plt.title(\"U²-NetP Saliency Map (Full ERP)\")\n",
        "    #plt.axis(\"off\")\n",
        "    #plt.show()\n",
        "\n",
        "    return saliency_full_resized\n",
        "\n",
        "# Step 5: Turn Salient Image into Tiles\n",
        "\n",
        "def saliency_to_rectified_tiles(saliency_map, widthDegree=20, heightDegree=20):\n",
        "    H, W = saliency_map.shape\n",
        "    num_lon_tiles = int(360 / widthDegree)\n",
        "    num_lat_tiles = int(180 / heightDegree)\n",
        "\n",
        "    tileWidth = int(round(W / num_lon_tiles))\n",
        "    tileHeight = int(round(H / num_lat_tiles))\n",
        "\n",
        "    tiles = [[None for _ in range(num_lon_tiles)] for _ in range(num_lat_tiles)]\n",
        "\n",
        "    for lat_i in range(num_lat_tiles):\n",
        "        lat_max = 90.0 - lat_i * heightDegree\n",
        "        lat_min = lat_max - heightDegree\n",
        "        lat_vals = np.linspace(lat_max, lat_min, tileHeight)\n",
        "\n",
        "        for lon_j in range(num_lon_tiles):\n",
        "            lon_min = lon_j * widthDegree - 180.0\n",
        "            lon_max = lon_min + widthDegree\n",
        "            lon_vals = np.linspace(lon_min, lon_max, tileWidth)\n",
        "\n",
        "            erp_x_vec = (lon_vals + 180.0) / 360.0 * (W - 1)\n",
        "            erp_y_vec = (90.0 - lat_vals) / 180.0 * (H - 1)\n",
        "\n",
        "            map_x = np.tile(erp_x_vec, (tileHeight, 1)).astype(np.float32)\n",
        "            map_y = np.repeat(erp_y_vec[:, np.newaxis], tileWidth, axis=1).astype(np.float32)\n",
        "\n",
        "            tile_sal = cv2.remap(\n",
        "                saliency_map.astype(np.float32),\n",
        "                map_x,\n",
        "                map_y,\n",
        "                interpolation=cv2.INTER_LINEAR,\n",
        "                borderMode=cv2.BORDER_WRAP\n",
        "            )\n",
        "\n",
        "            tiles[lat_i][lon_j] = tile_sal\n",
        "\n",
        "    return tiles\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# RAFT SETUP (Using HaniKamran/RAFT Fork)\n",
        "# ============================================================\n",
        "\n",
        "import os, sys\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "\n",
        "# ---------------------------\n",
        "# Setup working directory and Clone your Patched Repo\n",
        "# ---------------------------\n",
        "os.chdir(\"/content\")\n",
        "\n",
        "# Install minimal dependencies\n",
        "print(\"Installing dependencies...\")\n",
        "# Ensure scipy is installed correctly to avoid earlier conflicts\n",
        "!pip install -q opencv-python matplotlib Pillow einops scipy\n",
        "\n",
        "# Remove old RAFT if exists and clone YOUR FORK\n",
        "!rm -rf /content/RAFT\n",
        "# Cloning your specific patched branch: HaniKamran-patch-1\n",
        "!git clone https://github.com/HaniKamran/RAFT.git /content/RAFT\n",
        "%cd /content/RAFT\n",
        "!git checkout HaniKamran-patch-1\n",
        "%cd /content\n",
        "\n",
        "# Add RAFT core modules to path\n",
        "sys.path.append(\"/content/RAFT/core\")\n",
        "\n",
        "# ---------------------------\n",
        "# Import Patched RAFT Modules\n",
        "# ---------------------------\n",
        "try:\n",
        "    from raft import RAFT\n",
        "    from utils.utils import InputPadder\n",
        "    print(\"✓ Successfully imported core RAFT modules from your fork.\")\n",
        "except ImportError as e:\n",
        "    print(f\"FATAL ERROR: Could not import RAFT modules. Check your fork structure: {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Load CPU-only RAFT model\n",
        "# ---------------------------\n",
        "# These arguments trigger the 'small' model and disable CUDA-specific features.\n",
        "# The custom logic in your raft.py will ensure the CPUCostVolume is used here.\n",
        "args = argparse.Namespace(\n",
        "    small=True,\n",
        "    mixed_precision=False,\n",
        "    alternate_corr=False, # Set to False to use the primary CorrBlock path (which you patched)\n",
        "    dropout=0,\n",
        "    dropout2=0,\n",
        ")\n",
        "\n",
        "weights_path = \"/content/RAFT/models/raft-small.pth\"\n",
        "\n",
        "# Download weights if missing\n",
        "if not os.path.exists(weights_path):\n",
        "    print(\"Downloading RAFT weights...\")\n",
        "    %cd /content/RAFT\n",
        "    !bash download_models.sh\n",
        "    %cd /content\n",
        "\n",
        "print(\"Loading RAFT CPU model...\")\n",
        "# Initialize model on CPU\n",
        "model = RAFT(args).cpu()\n",
        "\n",
        "# Clean state_dict and load weights\n",
        "checkpoint = torch.load(weights_path, map_location=\"cpu\")\n",
        "state_dict = checkpoint.get(\"state_dict\", checkpoint)\n",
        "clean_state = {}\n",
        "for k, v in state_dict.items():\n",
        "    new_k = k.replace(\"module.\", \"\")\n",
        "    if new_k in model.state_dict():\n",
        "        clean_state[new_k] = v\n",
        "\n",
        "model.load_state_dict(clean_state, strict=False)\n",
        "model.eval()\n",
        "print(\"✓ RAFT model loaded on CPU successfully.\")\n",
        "\n",
        "# ---------------------------\n",
        "# CPU-safe optical flow function\n",
        "# ---------------------------\n",
        "def compute_flow_debug(img1, img2, model):\n",
        "    \"\"\"Computes optical flow using the loaded RAFT model across various scales.\"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    original_h, original_w = img1.shape[:2]\n",
        "    # Use smaller scales for better CPU stability and speed\n",
        "    scales = [1.0, 0.75, 0.5, 0.33, 0.25, 0.1, .01]\n",
        "\n",
        "    for s in scales:\n",
        "        try:\n",
        "            print(f\"\\nTrying scale {s}...\")\n",
        "            # Ensure dimensions are at least 32x32 and multiples of 8 (padding will handle multiples of 8)\n",
        "            new_h, new_w = max(32, int(original_h * s)), max(32, int(original_w * s))\n",
        "            im1 = cv2.resize(img1, (new_w, new_h))\n",
        "            im2 = cv2.resize(img2, (new_w, new_h))\n",
        "\n",
        "            # Convert to tensor and normalize [0, 1]\n",
        "            t1 = torch.from_numpy(im1/255.).permute(2,0,1).float().unsqueeze(0)\n",
        "            t2 = torch.from_numpy(im2/255.).permute(2,0,1).float().unsqueeze(0)\n",
        "\n",
        "            # Pad to multiples of 8 (required by RAFT architecture)\n",
        "            padder = InputPadder(t1.shape)\n",
        "            t1, t2 = padder.pad(t1, t2)\n",
        "\n",
        "            print(f\"Padded tensor shapes: t1={t1.shape}, t2={t2.shape}\")\n",
        "\n",
        "            # Run RAFT\n",
        "            with torch.no_grad():\n",
        "                # Setting iters=12 is faster for inference than the default 32\n",
        "                _, flow_up = model(t1, t2, iters=12, test_mode=True)\n",
        "\n",
        "            # Post-process flow: Resize back to original resolution (H, W)\n",
        "            flow = flow_up[0].permute(1,2,0).cpu().numpy()\n",
        "\n",
        "            # The flow values need to be scaled correctly after resizing\n",
        "            flow_x = cv2.resize(flow[...,0], (original_w, original_h), interpolation=cv2.INTER_LINEAR) * (original_w / flow.shape[1])\n",
        "            flow_y = cv2.resize(flow[...,1], (original_w, original_h), interpolation=cv2.INTER_LINEAR) * (original_h / flow.shape[0])\n",
        "            flow_final = np.stack([flow_x, flow_y], axis=-1)\n",
        "\n",
        "            print(f\"✓ Flow computed successfully at scale {s}\")\n",
        "            return flow_final\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Failed at scale {s} with error: {e}\")\n",
        "\n",
        "    raise RuntimeError(\"Flow failed at all scales\")\n",
        "\n",
        "print(\"\\n✓ RAFT CPU flow ready for execution. You can now call the function.\")\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "\n",
        "def flow_to_magnitude_map(flow):\n",
        "    \"\"\"\n",
        "    Computes only the magnitude (speed) of the optical flow.\n",
        "    Returns a normalized single-channel array.\n",
        "    \"\"\"\n",
        "    u = flow[:, :, 0]\n",
        "    v = flow[:, :, 1]\n",
        "\n",
        "    rad = np.sqrt(u**2 + v**2)\n",
        "    flow_max = np.max(rad)\n",
        "    if flow_max > 0:\n",
        "        rad /= flow_max\n",
        "\n",
        "    return rad.astype(np.float32)\n",
        "\n",
        "\n",
        "def generate_flow_heatmap(flow):\n",
        "    \"\"\"\n",
        "    Returns the heatmap image (RGB uint8 array) for the flow magnitude.\n",
        "    Does NOT display the heatmap.\n",
        "    \"\"\"\n",
        "    magnitude_map = flow_to_magnitude_map(flow)\n",
        "\n",
        "    # Apply 'jet' colormap using matplotlib\n",
        "    colormap = cm.get_cmap('jet')\n",
        "    heatmap = colormap(magnitude_map)  # RGBA float32 in [0,1]\n",
        "\n",
        "    # Convert RGBA float [0,1] → RGB uint8 [0,255]\n",
        "    heatmap = (heatmap[:, :, :3] * 255).astype(np.uint8)\n",
        "\n",
        "    return heatmap\n",
        "\n",
        "#heatmap_img = generate_flow_heatmap(flow)\n",
        "\n",
        "#plt.imshow(heatmap_img)\n",
        "#plt.title(\"Flow Heatmap (Returned Variable)\")\n",
        "#plt.axis(\"off\")\n",
        "#plt.show()\n",
        "\n",
        "\n",
        "# Example usage SalNet:\n",
        "# SalientStaticImage = process_image(Whereever image variable is)                    --- Image -> ERP Image -> ERP HeatMap\n",
        "# SalientStaticImageToTiles = saliency_to_rectified_tiles(saliency_full_resized)     --- ERP HeatMap -> ERP HeatMap Tiles\n",
        "\n",
        "# Example usage FlowNet:\n",
        "# img1 = cv2.cvtColor(cv2.imread(\"/content/frame1.jpg\"), cv2.COLOR_BGR2RGB)          --- Frame 1\n",
        "# img2 = cv2.cvtColor(cv2.imread(\"/content/frame2.jpg\"), cv2.COLOR_BGR2RGB)          --- Frame 2\n",
        "# flow = compute_flow(img1, img2, model)                                             --- Flow between Frame 1 & 2\n",
        "# heatmap_img = generate_flow_heatmap(flow)                                          --- HeatMap from Flow"
      ],
      "metadata": {
        "id": "pzzHBCjsIGv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1D head position index data processing"
      ],
      "metadata": {
        "id": "mo0ntUImMxqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def process_head_tracking_data(input_csv_path, participant_id, video_name,\n",
        "                                fps=60, frame_interval=5, cols=16, rows=9):\n",
        "    \"\"\"\n",
        "    Process head tracking data and convert to tile indices\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    input_csv_path : str\n",
        "        Path to the input CSV file\n",
        "    participant_id : int\n",
        "        ID of the participant to filter\n",
        "    video_name : int\n",
        "        Video identifier to filter\n",
        "    fps : int\n",
        "        Frames per second (default: 60)\n",
        "    frame_interval : int\n",
        "        Take every nth frame (default: 5)\n",
        "    cols : int\n",
        "        Number of columns in tile grid (default: 16)\n",
        "    rows : int\n",
        "        Number of rows in tile grid (default: 9)\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    tile_indices_array : numpy array\n",
        "        1D array of tile indices\n",
        "    target_timestamps : numpy array\n",
        "        Corresponding timestamps for each tile index\n",
        "    \"\"\"\n",
        "\n",
        "    # Read the CSV file\n",
        "    print(f\"Reading CSV file: {input_csv_path}\")\n",
        "    df = pd.read_csv(input_csv_path)\n",
        "\n",
        "    # Filter for specific participant and video\n",
        "    filtered_df = df[(df['id'] == participant_id) & (df['video'] == video_name)]\n",
        "\n",
        "    if len(filtered_df) == 0:\n",
        "        raise ValueError(f\"No data found for participant {participant_id} and video {video_name}\")\n",
        "\n",
        "    # Check stereo mode\n",
        "    stereo_mode = filtered_df['stereo'].iloc[0]\n",
        "    mode_str = \"stereo\" if stereo_mode else \"mono\"\n",
        "\n",
        "    # Sort by timestamp to ensure correct ordering\n",
        "    filtered_df = filtered_df.sort_values('t').reset_index(drop=True)\n",
        "\n",
        "    print(f\"\\nProcessing:\")\n",
        "    print(f\"  Participant ID: {participant_id}\")\n",
        "    print(f\"  Video: {video_name}\")\n",
        "    print(f\"  Mode: {mode_str}\")\n",
        "    print(f\"  Samples in filtered data: {len(filtered_df)}\")\n",
        "\n",
        "    # Parameters\n",
        "    time_interval = frame_interval / fps  # Time between samples\n",
        "\n",
        "    # Function to convert (u, v) coordinates to tile index\n",
        "    def uv_to_tile_index(u, v):\n",
        "        \"\"\"Convert normalized coordinates (u, v) to tile index\"\"\"\n",
        "        col = int(u * cols)\n",
        "        row = int(v * rows)\n",
        "\n",
        "        # Clamp to valid range\n",
        "        col = min(col, cols - 1)\n",
        "        row = min(row, rows - 1)\n",
        "\n",
        "        # Calculate tile index (row-major order)\n",
        "        tile_index = row * cols + col\n",
        "        return tile_index\n",
        "\n",
        "    # Get video duration (max timestamp)\n",
        "    max_time = filtered_df['t'].max()\n",
        "\n",
        "    # Generate target timestamps (every nth frame)\n",
        "    target_timestamps = np.arange(0, max_time + time_interval, time_interval)\n",
        "\n",
        "    print(f\"  Video duration: {max_time:.2f} seconds\")\n",
        "    print(f\"  Target timestamps: {len(target_timestamps)} (every {frame_interval} frames)\")\n",
        "\n",
        "    # Find closest timestamp in data for each target timestamp\n",
        "    tile_indices = []\n",
        "\n",
        "    for target_t in target_timestamps:\n",
        "        # Find the row with closest timestamp\n",
        "        idx = (filtered_df['t'] - target_t).abs().idxmin()\n",
        "\n",
        "        # Get u, v coordinates\n",
        "        u = filtered_df.loc[idx, 'u']\n",
        "        v = filtered_df.loc[idx, 'v']\n",
        "\n",
        "        # Convert to tile index\n",
        "        tile_idx = uv_to_tile_index(u, v)\n",
        "        tile_indices.append(tile_idx)\n",
        "\n",
        "    # Convert to numpy array\n",
        "    tile_indices_array = np.array(tile_indices)\n",
        "\n",
        "    print(f\"\\nGenerated {len(tile_indices_array)} tile indices\")\n",
        "    print(f\"Tile index range: {tile_indices_array.min()} to {tile_indices_array.max()}\")\n",
        "\n",
        "    return tile_indices_array, target_timestamps, mode_str\n",
        "\n",
        "\n",
        "def save_tile_indices_to_csv(tile_indices, timestamps, participant_id, video_name,\n",
        "                              mode_str, fps, frame_interval, output_dir='.'):\n",
        "    \"\"\"\n",
        "    Save tile indices to CSV file with proper naming (includes metadata)\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    tile_indices : numpy array\n",
        "        1D array of tile indices\n",
        "    timestamps : numpy array\n",
        "        Corresponding timestamps\n",
        "    participant_id : int\n",
        "        ID of the participant\n",
        "    video_name : int\n",
        "        Video identifier\n",
        "    mode_str : str\n",
        "        'mono' or 'stereo'\n",
        "    fps : int\n",
        "        Frames per second\n",
        "    frame_interval : int\n",
        "        Frame interval used\n",
        "    output_dir : str\n",
        "        Output directory (default: current directory)\n",
        "    \"\"\"\n",
        "\n",
        "    # Format video name with leading zeros (e.g., 1 -> 0001)\n",
        "    video_str = f\"{video_name:04d}\"\n",
        "\n",
        "    # Create filename\n",
        "    filename = f\"id{participant_id}_head_video_{video_str}_{mode_str}_{fps}fps_{frame_interval}th_frames.csv\"\n",
        "    output_path = os.path.join(output_dir, filename)\n",
        "\n",
        "    # Create DataFrame with tile indices and timestamps\n",
        "    output_df = pd.DataFrame({\n",
        "        'frame_index': np.arange(0, len(tile_indices) * frame_interval, frame_interval),\n",
        "        'timestamp': timestamps,\n",
        "        'tile_index': tile_indices\n",
        "    })\n",
        "\n",
        "    # Save to CSV\n",
        "    output_df.to_csv(output_path, index=False)\n",
        "    print(f\"\\nOutput saved to: {output_path}\")\n",
        "    print(f\"Output shape: {output_df.shape}\")\n",
        "    print(f\"\\nFirst 10 rows of output:\")\n",
        "    print(output_df.head(10))\n",
        "\n",
        "    return output_path\n",
        "\n",
        "\n",
        "def save_tile_indices_cnn_format(tile_indices, participant_id, video_name,\n",
        "                                  mode_str, fps, frame_interval, output_dir='.'):\n",
        "    \"\"\"\n",
        "    Save tile indices as a single line CSV for CNN input (no metadata, no headers)\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    tile_indices : numpy array\n",
        "        1D array of tile indices\n",
        "    participant_id : int\n",
        "        ID of the participant\n",
        "    video_name : int\n",
        "        Video identifier\n",
        "    mode_str : str\n",
        "        'mono' or 'stereo'\n",
        "    fps : int\n",
        "        Frames per second\n",
        "    frame_interval : int\n",
        "        Frame interval used\n",
        "    output_dir : str\n",
        "        Output directory (default: current directory)\n",
        "    \"\"\"\n",
        "\n",
        "    # Format video name with leading zeros (e.g., 1 -> 0001)\n",
        "    video_str = f\"{video_name:04d}\"\n",
        "\n",
        "    # Create filename with _cnn suffix\n",
        "    filename = f\"id{participant_id}_head_video_{video_str}_{mode_str}_{fps}fps_{frame_interval}th_frames_cnn.csv\"\n",
        "    output_path = os.path.join(output_dir, filename)\n",
        "\n",
        "    # Save as single line without header or index\n",
        "    np.savetxt(output_path, tile_indices.reshape(1, -1), delimiter=',', fmt='%d')\n",
        "\n",
        "    print(f\"\\nCNN format output saved to: {output_path}\")\n",
        "    print(f\"Array length: {len(tile_indices)}\")\n",
        "    print(f\"First 20 tile indices: {tile_indices[:20]}\")\n",
        "\n",
        "    return output_path\n",
        "\n",
        "\n",
        "# ==================== MAIN EXECUTION ====================\n",
        "\n",
        "# Configuration\n",
        "INPUT_CSV_PATH = 'your_data.csv'  # Change this to your CSV file path\n",
        "PARTICIPANT_ID = 11\n",
        "VIDEO_NAME = 1  # Video 001\n",
        "FPS = 60\n",
        "FRAME_INTERVAL = 5\n",
        "TILE_COLS = 16\n",
        "TILE_ROWS = 9\n",
        "\n",
        "# For Google Colab: Upload file\n",
        "print(\"Upload your CSV file:\")\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "INPUT_CSV_PATH = list(uploaded.keys())[0]\n",
        "\n",
        "# Process the data\n",
        "tile_indices, timestamps, mode_str = process_head_tracking_data(\n",
        "    input_csv_path=INPUT_CSV_PATH,\n",
        "    participant_id=PARTICIPANT_ID,\n",
        "    video_name=VIDEO_NAME,\n",
        "    fps=FPS,\n",
        "    frame_interval=FRAME_INTERVAL,\n",
        "    cols=TILE_COLS,\n",
        "    rows=TILE_ROWS\n",
        ")\n",
        "\n",
        "# Save to CSV with metadata (original format)\n",
        "output_path = save_tile_indices_to_csv(\n",
        "    tile_indices=tile_indices,\n",
        "    timestamps=timestamps,\n",
        "    participant_id=PARTICIPANT_ID,\n",
        "    video_name=VIDEO_NAME,\n",
        "    mode_str=mode_str,\n",
        "    fps=FPS,\n",
        "    frame_interval=FRAME_INTERVAL\n",
        ")\n",
        "\n",
        "# Save to CSV for CNN input (single line, no headers)\n",
        "output_path_cnn = save_tile_indices_cnn_format(\n",
        "    tile_indices=tile_indices,\n",
        "    participant_id=PARTICIPANT_ID,\n",
        "    video_name=VIDEO_NAME,\n",
        "    mode_str=mode_str,\n",
        "    fps=FPS,\n",
        "    frame_interval=FRAME_INTERVAL\n",
        ")\n",
        "\n",
        "# Optional: Visualize\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(timestamps, tile_indices, marker='o', markersize=2, linestyle='-', linewidth=0.5)\n",
        "plt.xlabel('Time (seconds)')\n",
        "plt.ylabel('Tile Index')\n",
        "plt.title(f'Tile Indices Over Time\\n(ID={PARTICIPANT_ID}, Video={VIDEO_NAME:04d})')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.hist(tile_indices, bins=TILE_COLS * TILE_ROWS, range=(0, TILE_COLS * TILE_ROWS))\n",
        "plt.xlabel('Tile Index')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Viewed Tiles')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "# Visualize as heatmap\n",
        "tile_counts = np.bincount(tile_indices, minlength=TILE_COLS * TILE_ROWS)\n",
        "heatmap = tile_counts.reshape(TILE_ROWS, TILE_COLS)\n",
        "plt.imshow(heatmap, cmap='hot', interpolation='nearest', aspect='auto')\n",
        "plt.colorbar(label='View Count')\n",
        "plt.xlabel('Tile Column')\n",
        "plt.ylabel('Tile Row')\n",
        "plt.title('Heatmap of Viewed Tiles')\n",
        "plt.xticks(range(0, TILE_COLS, 2))\n",
        "plt.yticks(range(TILE_ROWS))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Download both output files (for Colab)\n",
        "# print(\"\\n\" + \"=\"*60)\n",
        "# print(\"Downloading output files...\")\n",
        "# print(\"=\"*60)\n",
        "# files.download(output_path)\n",
        "# files.download(output_path_cnn)\n",
        "# ```\n",
        "\n",
        "# **Now you get TWO output files:**\n",
        "\n",
        "# 1. **`id11_head_video_0001_mono_60fps_5th_frames.csv`** (with metadata):\n",
        "# ```\n",
        "# frame_index,timestamp,tile_index\n",
        "# 0,0.0,67\n",
        "# 5,0.0833333,68\n",
        "# 10,0.1666667,69\n",
        "# ...\n",
        "# ```\n",
        "\n",
        "# 2. **`id11_head_video_0001_mono_60fps_5th_frames_cnn.csv`** (for CNN input):\n",
        "# ```\n",
        "# 67,68,69,70,71,68,67,65,64,63,62,61,60,59,58,57,56,55,54,53,..."
      ],
      "metadata": {
        "id": "kWyFe4q5M9e6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN"
      ],
      "metadata": {
        "id": "uqNYx0bw24KJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWa1egndri88",
        "outputId": "9f137977-67bb-4410-c8a1-05cd306ef1b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "\n",
            "============================================================\n",
            "360° Video Saliency-Based Tile Predictor\n",
            "============================================================\n",
            "\n",
            "Configuration:\n",
            "  Frame size: 960x480 (25% of 4K)\n",
            "  Grid: 16x9 = 144 tiles\n",
            "  Tile size: 60x53\n",
            "  Heatmaps per frame: 9\n",
            "  Video: 3s @ 60fps = 180 total frames\n",
            "  Sampling: Every 5th frame = 36 sampled frames\n",
            "\n",
            "\n",
            "============================================================\n",
            "Generating Synthetic Video Data\n",
            "============================================================\n",
            "\n",
            "Videos: 1\n",
            "Duration: 3s @ 60 FPS = 180 total frames\n",
            "Sampling: Every 5th frame = 36 sampled frames\n",
            "Heatmap resolution: 960x480 (25% of 4K)\n",
            "Total sampled frames: 36\n",
            "\n",
            "Generating video 1/1...\n",
            "\n",
            "Data generation complete!\n",
            "  Heatmaps 4D shape: (1, 36, 9, 480, 960)\n",
            "  Tile indices shape: (1, 36)\n",
            "  Memory usage: 597.2 MB\n",
            "\n",
            "============================================================\n",
            "Data Structure:\n",
            "============================================================\n",
            "4D Heatmaps Matrix: (1, 36, 9, 480, 960)\n",
            "  [videos, sampled_frames, heatmaps, height, width]\n",
            "  [1, 36, 9, 480, 960]\n",
            "  - 1 video\n",
            "  - 36 sampled frames (every 5th frame from 180 total @ 60 FPS)\n",
            "  - 9 heatmaps per frame (7 audio + 2 video)\n",
            "  - 480x960 resolution (25% of 4K)\n",
            "\n",
            "1D Tile Index Array: (1, 36)\n",
            "  [videos, sampled_frames]\n",
            "  [1, 36]\n",
            "  - 1 user's viewing data\n",
            "  - 36 tile indices (sampled every 5th frame, range 0-143)\n",
            "  - Each index represents which tile user is looking at\n",
            "============================================================\n",
            "\n",
            "Flattened for Training:\n",
            "  Heatmaps: (36, 9, 480, 960)\n",
            "  Tile indices: (36,)\n",
            "\n",
            "Dataset splits:\n",
            "  Train: 25 | Val: 5 | Test: 6\n",
            "\n",
            "Model parameters: 660,864\n",
            "\n",
            "Training started...\n",
            "\n",
            "Epoch 1/15: Train 4.0% | Val 20.0% | Dist 1.20 | 7.2s\n",
            "Epoch 2/15: Train 52.0% | Val 20.0% | Dist 1.20 | 5.9s\n",
            "Epoch 3/15: Train 48.0% | Val 20.0% | Dist 1.20 | 6.4s\n",
            "Epoch 4/15: Train 52.0% | Val 20.0% | Dist 1.20 | 5.9s\n",
            "Epoch 5/15: Train 68.0% | Val 20.0% | Dist 1.20 | 6.4s\n",
            "Epoch 6/15: Train 64.0% | Val 20.0% | Dist 1.20 | 6.0s\n",
            "Epoch 7/15: Train 96.0% | Val 20.0% | Dist 1.20 | 6.3s\n",
            "Epoch 8/15: Train 92.0% | Val 20.0% | Dist 1.20 | 6.0s\n",
            "Epoch 9/15: Train 96.0% | Val 40.0% | Dist 1.00 | 6.2s\n",
            "Epoch 10/15: Train 96.0% | Val 20.0% | Dist 1.20 | 7.1s\n",
            "Epoch 11/15: Train 96.0% | Val 20.0% | Dist 1.20 | 6.4s\n",
            "Epoch 12/15: Train 100.0% | Val 20.0% | Dist 1.20 | 5.8s\n",
            "Epoch 13/15: Train 100.0% | Val 40.0% | Dist 1.00 | 6.4s\n",
            "Epoch 14/15: Train 100.0% | Val 40.0% | Dist 1.00 | 5.9s\n",
            "Epoch 15/15: Train 100.0% | Val 40.0% | Dist 1.00 | 6.3s\n",
            "\n",
            "Evaluating on test set...\n",
            "\n",
            "============================================================\n",
            "Training Complete!\n",
            "============================================================\n",
            "\n",
            "Best Val Accuracy: 40.00%\n",
            "Test Accuracy: 16.67%\n",
            "Test Avg Tile Distance: 1.63 tiles\n",
            "Total Training Time: 94.4s (1.6 min)\n",
            "\n",
            "  Epoch    Train Loss  Train Acc      Val Loss  Val Acc      Tile Dist  Time\n",
            "-------  ------------  -----------  ----------  ---------  -----------  ------\n",
            "      1        4.8217  4.0%             4.9287  20.0%              1.2  7.2s\n",
            "      2        3.3732  52.0%            4.7709  20.0%              1.2  5.9s\n",
            "      3        2.1772  48.0%            4.4743  20.0%              1.2  6.4s\n",
            "      4        1.6471  52.0%            4.1378  20.0%              1.2  5.9s\n",
            "      5        1.1959  68.0%            3.8367  20.0%              1.2  6.4s\n",
            "      6        0.8666  64.0%            3.5611  20.0%              1.2  6.0s\n",
            "      7        0.4878  96.0%            3.3052  20.0%              1.2  6.3s\n",
            "      8        0.4818  92.0%            3.0828  20.0%              1.2  6.0s\n",
            "      9        0.4599  96.0%            2.9296  40.0%              1    6.2s\n",
            "     10        0.2035  96.0%            2.8579  20.0%              1.2  7.1s\n",
            "     11        0.1899  96.0%            2.8745  20.0%              1.2  6.4s\n",
            "     12        0.1143  100.0%           2.9411  20.0%              1.2  5.8s\n",
            "     13        0.1516  100.0%           3.0022  40.0%              1    6.4s\n",
            "     14        0.0717  100.0%           3.0762  40.0%              1    5.9s\n",
            "     15        0.0295  100.0%           3.1433  40.0%              1    6.3s\n",
            "\n",
            "============================================================\n",
            "Inference Speed Test\n",
            "============================================================\n",
            "\n",
            "Average inference: 28.34ms\n",
            "Throughput: 35.3 FPS\n",
            "Can process: 0.6x realtime\n",
            "Sample prediction: Tile 54 at (6, 3)\n",
            "\n",
            "============================================================\n",
            "Model saved as 'best_model.pth'\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "User Viewing Data - Video 0 (3 seconds, every 5th frame)\n",
            "============================================================\n",
            "Sample# | Frame# | Time(s) | Tile Index | Tile Grid (x,y)\n",
            "------------------------------------------------------------\n",
            "      0 |      0 |   0.00  |         54 | ( 6,  3)\n",
            "      3 |     15 |   0.25  |         53 | ( 5,  3)\n",
            "      6 |     30 |   0.50  |         70 | ( 6,  4)\n",
            "      9 |     45 |   0.75  |         55 | ( 7,  3)\n",
            "     12 |     60 |   1.00  |         55 | ( 7,  3)\n",
            "     15 |     75 |   1.25  |         56 | ( 8,  3)\n",
            "     18 |     90 |   1.50  |         56 | ( 8,  3)\n",
            "     21 |    105 |   1.75  |         56 | ( 8,  3)\n",
            "     24 |    120 |   2.00  |         73 | ( 9,  4)\n",
            "     27 |    135 |   2.25  |         74 | (10,  4)\n",
            "     30 |    150 |   2.50  |         58 | (10,  3)\n",
            "     33 |    165 |   2.75  |         75 | (11,  4)\n",
            "============================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision tqdm tabulate --quiet\n",
        "\n",
        "import time, os, torch, torch.nn as nn, torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import numpy as np\n",
        "from tabulate import tabulate\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "\n",
        "# Clear GPU memory\n",
        "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "if device == \"cuda\":\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\\n\")\n",
        "\n",
        "# Configuration - 25% of 4K resolution\n",
        "FRAME_WIDTH = 960    # 25% of 3840\n",
        "FRAME_HEIGHT = 480   # 25% of 1920\n",
        "TILES_X = 16\n",
        "TILES_Y = 9\n",
        "NUM_TILES = TILES_X * TILES_Y  # 144 tiles\n",
        "TILE_WIDTH = FRAME_WIDTH // TILES_X   # 60 pixels\n",
        "TILE_HEIGHT = FRAME_HEIGHT // TILES_Y  # ~53 pixels\n",
        "NUM_HEATMAPS = 9  # 7 audio + 2 video\n",
        "\n",
        "# Video configuration\n",
        "FPS = 60  # Frames per second (full framerate)\n",
        "VIDEO_DURATION = 3  # seconds\n",
        "FRAME_SAMPLE_RATE = 5  # Take every 5th frame\n",
        "TOTAL_FRAMES = FPS * VIDEO_DURATION  # 180 frames total for 3 seconds\n",
        "FRAMES_PER_VIDEO = TOTAL_FRAMES // FRAME_SAMPLE_RATE  # 36 sampled frames\n",
        "\n",
        "class SaliencyTileDataset(Dataset):\n",
        "    \"\"\"Memory-efficient dataset for saliency heatmaps\"\"\"\n",
        "\n",
        "    def __init__(self, heatmaps, tile_indices):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            heatmaps: numpy array of shape (N_frames, NUM_HEATMAPS, H, W)\n",
        "            tile_indices: numpy array of shape (N_frames,) with tile indices [0-143]\n",
        "        \"\"\"\n",
        "        # Convert to float32 for memory efficiency\n",
        "        self.heatmaps = heatmaps.astype(np.float32)\n",
        "        self.tile_indices = tile_indices.astype(np.int64)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tile_indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        heatmap = torch.from_numpy(self.heatmaps[idx])\n",
        "        tile_idx = torch.tensor(self.tile_indices[idx], dtype=torch.long)\n",
        "        return heatmap, tile_idx\n",
        "\n",
        "\n",
        "class HeatmapFusionCNN(nn.Module):\n",
        "    \"\"\"Lightweight CNN for fusing saliency heatmaps\"\"\"\n",
        "\n",
        "    def __init__(self, num_heatmaps=9, num_tiles=144, dropout=0.3):\n",
        "        super(HeatmapFusionCNN, self).__init__()\n",
        "\n",
        "        # Heatmap fusion with 1x1 conv\n",
        "        self.fusion = nn.Conv2d(num_heatmaps, 8, kernel_size=1)\n",
        "\n",
        "        # Lightweight feature extraction\n",
        "        self.conv1 = nn.Conv2d(8, 32, kernel_size=5, stride=4, padding=2)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "\n",
        "        # Adaptive pooling\n",
        "        self.pool = nn.AdaptiveAvgPool2d((4, 4))\n",
        "\n",
        "        # Classifier\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 256)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc2 = nn.Linear(256, num_tiles)\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Fuse heatmaps\n",
        "        x = self.fusion(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        # Feature extraction\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        # Pool and classify\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def tile_coords_to_index(x, y, tiles_x=TILES_X):\n",
        "    \"\"\"Convert tile coordinates to linear index\"\"\"\n",
        "    return y * tiles_x + x\n",
        "\n",
        "\n",
        "def tile_index_to_coords(idx, tiles_x=TILES_X):\n",
        "    \"\"\"Convert linear index to tile coordinates\"\"\"\n",
        "    y = idx // tiles_x\n",
        "    x = idx % tiles_x\n",
        "    return x, y\n",
        "\n",
        "\n",
        "def tile_distance(pred_idx, true_idx, tiles_x=TILES_X):\n",
        "    \"\"\"Calculate tile distance\"\"\"\n",
        "    px, py = tile_index_to_coords(pred_idx, tiles_x)\n",
        "    tx, ty = tile_index_to_coords(true_idx, tiles_x)\n",
        "    return np.sqrt((px - tx)**2 + (py - ty)**2)\n",
        "\n",
        "\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for heatmaps, tile_indices in dataloader:\n",
        "        heatmaps = heatmaps.to(device)\n",
        "        tile_indices = tile_indices.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(heatmaps)\n",
        "        loss = criterion(outputs, tile_indices)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += tile_indices.size(0)\n",
        "        correct += predicted.eq(tile_indices).sum().item()\n",
        "\n",
        "    return total_loss / len(dataloader), 100. * correct / total\n",
        "\n",
        "\n",
        "def validate(model, dataloader, criterion, device):\n",
        "    \"\"\"Validate the model\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    tile_distances = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for heatmaps, tile_indices in dataloader:\n",
        "            heatmaps = heatmaps.to(device)\n",
        "            tile_indices = tile_indices.to(device)\n",
        "\n",
        "            outputs = model(heatmaps)\n",
        "            loss = criterion(outputs, tile_indices)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += tile_indices.size(0)\n",
        "            correct += predicted.eq(tile_indices).sum().item()\n",
        "\n",
        "            for pred, true in zip(predicted.cpu().numpy(), tile_indices.cpu().numpy()):\n",
        "                tile_distances.append(tile_distance(pred, true))\n",
        "\n",
        "    avg_distance = np.mean(tile_distances)\n",
        "    return total_loss / len(dataloader), 100. * correct / total, avg_distance\n",
        "\n",
        "\n",
        "def generate_synthetic_video_data(num_videos=1):\n",
        "    \"\"\"\n",
        "    Generate synthetic video data at 60 FPS, sampling every 5th frame\n",
        "\n",
        "    Returns:\n",
        "        heatmaps_4d: shape (num_videos, FRAMES_PER_VIDEO, NUM_HEATMAPS, H, W)\n",
        "        tile_indices_1d: shape (num_videos, FRAMES_PER_VIDEO)\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Generating Synthetic Video Data\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "    print(f\"Videos: {num_videos}\")\n",
        "    print(f\"Duration: {VIDEO_DURATION}s @ {FPS} FPS = {TOTAL_FRAMES} total frames\")\n",
        "    print(f\"Sampling: Every {FRAME_SAMPLE_RATE}th frame = {FRAMES_PER_VIDEO} sampled frames\")\n",
        "    print(f\"Heatmap resolution: {FRAME_WIDTH}x{FRAME_HEIGHT} (25% of 4K)\")\n",
        "    print(f\"Total sampled frames: {num_videos * FRAMES_PER_VIDEO}\\n\")\n",
        "\n",
        "    # Initialize 4D matrix for heatmaps: (videos, sampled_frames, heatmaps, height, width)\n",
        "    heatmaps_4d = np.zeros((num_videos, FRAMES_PER_VIDEO, NUM_HEATMAPS,\n",
        "                            FRAME_HEIGHT, FRAME_WIDTH), dtype=np.float32)\n",
        "\n",
        "    # Initialize 1D matrix for tile indices: (videos, sampled_frames)\n",
        "    tile_indices_1d = np.zeros((num_videos, FRAMES_PER_VIDEO), dtype=np.int64)\n",
        "\n",
        "    for video_idx in range(num_videos):\n",
        "        print(f\"Generating video {video_idx + 1}/{num_videos}...\")\n",
        "\n",
        "        # Create a smooth trajectory for the salient region across ALL frames\n",
        "        start_x = np.random.randint(TILE_WIDTH * 2, FRAME_WIDTH - TILE_WIDTH * 2)\n",
        "        start_y = np.random.randint(TILE_HEIGHT * 2, FRAME_HEIGHT - TILE_HEIGHT * 2)\n",
        "\n",
        "        end_x = np.random.randint(TILE_WIDTH * 2, FRAME_WIDTH - TILE_WIDTH * 2)\n",
        "        end_y = np.random.randint(TILE_HEIGHT * 2, FRAME_HEIGHT - TILE_HEIGHT * 2)\n",
        "\n",
        "        # Create smooth trajectory for ALL 180 frames (60 FPS * 3 sec)\n",
        "        trajectory_x = np.linspace(start_x, end_x, TOTAL_FRAMES)\n",
        "        trajectory_y = np.linspace(start_y, end_y, TOTAL_FRAMES)\n",
        "\n",
        "        # Add jitter\n",
        "        jitter_x = np.random.randn(TOTAL_FRAMES) * (TILE_WIDTH / 4)\n",
        "        jitter_y = np.random.randn(TOTAL_FRAMES) * (TILE_HEIGHT / 4)\n",
        "\n",
        "        trajectory_x = np.clip(trajectory_x + jitter_x, 0, FRAME_WIDTH - 1)\n",
        "        trajectory_y = np.clip(trajectory_y + jitter_y, 0, FRAME_HEIGHT - 1)\n",
        "\n",
        "        # Sample every 5th frame\n",
        "        sampled_frame_idx = 0\n",
        "        for full_frame_idx in range(0, TOTAL_FRAMES, FRAME_SAMPLE_RATE):\n",
        "            # Current center of attention at this frame\n",
        "            cx = int(trajectory_x[full_frame_idx])\n",
        "            cy = int(trajectory_y[full_frame_idx])\n",
        "\n",
        "            # Generate heatmaps for this sampled frame\n",
        "            fused_saliency = np.zeros((FRAME_HEIGHT, FRAME_WIDTH), dtype=np.float32)\n",
        "\n",
        "            for heatmap_idx in range(NUM_HEATMAPS):\n",
        "                # Random sigma for variety in saliency spread\n",
        "                sigma = FRAME_WIDTH / (6 + np.random.rand() * 4)  # Varies between /6 and /10\n",
        "\n",
        "                # Add small random offset for each heatmap\n",
        "                offset_x = np.random.randint(-TILE_WIDTH//2, TILE_WIDTH//2)\n",
        "                offset_y = np.random.randint(-TILE_HEIGHT//2, TILE_HEIGHT//2)\n",
        "\n",
        "                center_x = np.clip(cx + offset_x, 0, FRAME_WIDTH - 1)\n",
        "                center_y = np.clip(cy + offset_y, 0, FRAME_HEIGHT - 1)\n",
        "\n",
        "                # Create Gaussian saliency map\n",
        "                y, x = np.ogrid[:FRAME_HEIGHT, :FRAME_WIDTH]\n",
        "                heatmap = np.exp(-((x - center_x)**2 + (y - center_y)**2) / (2 * sigma**2))\n",
        "\n",
        "                # Store heatmap\n",
        "                heatmaps_4d[video_idx, sampled_frame_idx, heatmap_idx] = heatmap\n",
        "\n",
        "                # Accumulate saliency (equal weight, let model learn optimal fusion)\n",
        "                fused_saliency += heatmap\n",
        "\n",
        "            # Find the tile with maximum saliency\n",
        "            max_y, max_x = np.unravel_index(fused_saliency.argmax(), fused_saliency.shape)\n",
        "            tile_x = min(max_x // TILE_WIDTH, TILES_X - 1)\n",
        "            tile_y = min(max_y // TILE_HEIGHT, TILES_Y - 1)\n",
        "            tile_idx = tile_coords_to_index(tile_x, tile_y)\n",
        "\n",
        "            # Store tile index for this sampled frame\n",
        "            tile_indices_1d[video_idx, sampled_frame_idx] = tile_idx\n",
        "\n",
        "            sampled_frame_idx += 1\n",
        "\n",
        "    print(f\"\\nData generation complete!\")\n",
        "    print(f\"  Heatmaps 4D shape: {heatmaps_4d.shape}\")\n",
        "    print(f\"  Tile indices shape: {tile_indices_1d.shape}\")\n",
        "    print(f\"  Memory usage: {heatmaps_4d.nbytes / 1e6:.1f} MB\\n\")\n",
        "\n",
        "    return heatmaps_4d, tile_indices_1d\n",
        "\n",
        "\n",
        "def flatten_video_data(heatmaps_4d, tile_indices_1d):\n",
        "    \"\"\"\n",
        "    Flatten video data from (videos, frames, ...) to (total_frames, ...)\n",
        "\n",
        "    Args:\n",
        "        heatmaps_4d: shape (num_videos, frames_per_video, num_heatmaps, H, W)\n",
        "        tile_indices_1d: shape (num_videos, frames_per_video)\n",
        "\n",
        "    Returns:\n",
        "        heatmaps: shape (total_frames, num_heatmaps, H, W)\n",
        "        tile_indices: shape (total_frames,)\n",
        "    \"\"\"\n",
        "    num_videos, frames_per_video = tile_indices_1d.shape\n",
        "    total_frames = num_videos * frames_per_video\n",
        "\n",
        "    # Reshape heatmaps: (videos, frames, heatmaps, H, W) -> (total_frames, heatmaps, H, W)\n",
        "    heatmaps = heatmaps_4d.reshape(total_frames, NUM_HEATMAPS, FRAME_HEIGHT, FRAME_WIDTH)\n",
        "\n",
        "    # Flatten tile indices: (videos, frames) -> (total_frames,)\n",
        "    tile_indices = tile_indices_1d.reshape(total_frames)\n",
        "\n",
        "    return heatmaps, tile_indices\n",
        "\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"360° Video Saliency-Based Tile Predictor\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    print(f\"Configuration:\")\n",
        "    print(f\"  Frame size: {FRAME_WIDTH}x{FRAME_HEIGHT} (25% of 4K)\")\n",
        "    print(f\"  Grid: {TILES_X}x{TILES_Y} = {NUM_TILES} tiles\")\n",
        "    print(f\"  Tile size: {TILE_WIDTH}x{TILE_HEIGHT}\")\n",
        "    print(f\"  Heatmaps per frame: {NUM_HEATMAPS}\")\n",
        "    print(f\"  Video: {VIDEO_DURATION}s @ {FPS}fps = {TOTAL_FRAMES} total frames\")\n",
        "    print(f\"  Sampling: Every {FRAME_SAMPLE_RATE}th frame = {FRAMES_PER_VIDEO} sampled frames\\n\")\n",
        "\n",
        "    # Generate synthetic video data - just 1 video for memory efficiency\n",
        "    num_videos = 1  # 1 video of 3 seconds\n",
        "    heatmaps_4d, tile_indices_1d = generate_synthetic_video_data(num_videos=num_videos)\n",
        "\n",
        "    print(f\"{'='*60}\")\n",
        "    print(\"Data Structure:\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"4D Heatmaps Matrix: {heatmaps_4d.shape}\")\n",
        "    print(f\"  [videos, sampled_frames, heatmaps, height, width]\")\n",
        "    print(f\"  [1, 36, 9, 480, 960]\")\n",
        "    print(f\"  - 1 video\")\n",
        "    print(f\"  - 36 sampled frames (every 5th frame from 180 total @ 60 FPS)\")\n",
        "    print(f\"  - 9 heatmaps per frame (7 audio + 2 video)\")\n",
        "    print(f\"  - 480x960 resolution (25% of 4K)\")\n",
        "    print(f\"\\n1D Tile Index Array: {tile_indices_1d.shape}\")\n",
        "    print(f\"  [videos, sampled_frames]\")\n",
        "    print(f\"  [1, 36]\")\n",
        "    print(f\"  - 1 user's viewing data\")\n",
        "    print(f\"  - 36 tile indices (sampled every 5th frame, range 0-143)\")\n",
        "    print(f\"  - Each index represents which tile user is looking at\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    # Flatten data for training\n",
        "    heatmaps_flat, tile_indices_flat = flatten_video_data(heatmaps_4d, tile_indices_1d)\n",
        "\n",
        "    print(f\"Flattened for Training:\")\n",
        "    print(f\"  Heatmaps: {heatmaps_flat.shape}\")\n",
        "    print(f\"  Tile indices: {tile_indices_flat.shape}\\n\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = SaliencyTileDataset(heatmaps_flat, tile_indices_flat)\n",
        "\n",
        "    # Split\n",
        "    train_size = int(0.7 * len(dataset))\n",
        "    val_size = int(0.15 * len(dataset))\n",
        "    test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "    train_dataset, val_dataset, test_dataset = random_split(\n",
        "        dataset, [train_size, val_size, test_size],\n",
        "        generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "\n",
        "    # Dataloaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=0)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=0)\n",
        "\n",
        "    print(f\"Dataset splits:\")\n",
        "    print(f\"  Train: {len(train_dataset)} | Val: {len(val_dataset)} | Test: {len(test_dataset)}\\n\")\n",
        "\n",
        "    # Initialize model\n",
        "    model = HeatmapFusionCNN(num_heatmaps=NUM_HEATMAPS, num_tiles=NUM_TILES).to(device)\n",
        "\n",
        "    # Count parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Model parameters: {total_params:,}\\n\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
        "\n",
        "    # Training\n",
        "    num_epochs = 15\n",
        "    best_val_acc = 0\n",
        "    results = []\n",
        "\n",
        "    print(\"Training started...\\n\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_start = time.time()\n",
        "\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        val_loss, val_acc, avg_tile_dist = validate(model, val_loader, criterion, device)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "        epoch_time = time.time() - epoch_start\n",
        "\n",
        "        results.append([\n",
        "            epoch + 1,\n",
        "            f\"{train_loss:.4f}\",\n",
        "            f\"{train_acc:.1f}%\",\n",
        "            f\"{val_loss:.4f}\",\n",
        "            f\"{val_acc:.1f}%\",\n",
        "            f\"{avg_tile_dist:.2f}\",\n",
        "            f\"{epoch_time:.1f}s\"\n",
        "        ])\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}: \"\n",
        "              f\"Train {train_acc:.1f}% | Val {val_acc:.1f}% | \"\n",
        "              f\"Dist {avg_tile_dist:.2f} | {epoch_time:.1f}s\")\n",
        "\n",
        "        # Clear cache periodically\n",
        "        if epoch % 3 == 0 and device == \"cuda\":\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "\n",
        "    # Test evaluation\n",
        "    print(\"\\nEvaluating on test set...\")\n",
        "    model.load_state_dict(torch.load('best_model.pth'))\n",
        "    test_loss, test_acc, test_tile_dist = validate(model, test_loader, criterion, device)\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"Training Complete!\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    print(f\"Best Val Accuracy: {best_val_acc:.2f}%\")\n",
        "    print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
        "    print(f\"Test Avg Tile Distance: {test_tile_dist:.2f} tiles\")\n",
        "    print(f\"Total Training Time: {total_time:.1f}s ({total_time/60:.1f} min)\\n\")\n",
        "\n",
        "    # Training history table\n",
        "    headers = ['Epoch', 'Train Loss', 'Train Acc', 'Val Loss', 'Val Acc', 'Tile Dist', 'Time']\n",
        "    print(tabulate(results, headers=headers, tablefmt='simple'))\n",
        "\n",
        "    # Inference speed test\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"Inference Speed Test\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    model.eval()\n",
        "    test_input = torch.randn(1, NUM_HEATMAPS, FRAME_HEIGHT, FRAME_WIDTH).to(device)\n",
        "\n",
        "    # Warmup\n",
        "    with torch.no_grad():\n",
        "        for _ in range(10):\n",
        "            _ = model(test_input)\n",
        "\n",
        "    # Measure\n",
        "    if device == \"cuda\":\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    inference_times = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(100):\n",
        "            start = time.time()\n",
        "            output = model(test_input)\n",
        "            if device == \"cuda\":\n",
        "                torch.cuda.synchronize()\n",
        "            inference_times.append(time.time() - start)\n",
        "\n",
        "    avg_inf_ms = np.mean(inference_times) * 1000\n",
        "    fps = 1000 / avg_inf_ms\n",
        "\n",
        "    print(f\"Average inference: {avg_inf_ms:.2f}ms\")\n",
        "    print(f\"Throughput: {fps:.1f} FPS\")\n",
        "    print(f\"Can process: {fps/FPS:.1f}x realtime\")\n",
        "\n",
        "    pred_tile = output.argmax(1).item()\n",
        "    x, y = tile_index_to_coords(pred_tile)\n",
        "    print(f\"Sample prediction: Tile {pred_tile} at ({x}, {y})\")\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"Model saved as 'best_model.pth'\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    # Sample video analysis - show all sampled frames\n",
        "    print(f\"{'='*60}\")\n",
        "    print(\"User Viewing Data - Video 0 (3 seconds, every 5th frame)\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Sample# | Frame# | Time(s) | Tile Index | Tile Grid (x,y)\")\n",
        "    print(f\"{'-'*60}\")\n",
        "    for i in range(FRAMES_PER_VIDEO):\n",
        "        tile_idx = tile_indices_1d[0, i]\n",
        "        tx, ty = tile_index_to_coords(tile_idx)\n",
        "        actual_frame = i * FRAME_SAMPLE_RATE\n",
        "        time_sec = actual_frame / FPS\n",
        "        if i % max(1, FRAMES_PER_VIDEO//12) == 0:  # Show ~12 samples\n",
        "            print(f\"{i:7d} | {actual_frame:6d} | {time_sec:6.2f}  | {tile_idx:10d} | ({tx:2d}, {ty:2d})\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    # Memory cleanup\n",
        "    del model, train_loader, val_loader, test_loader\n",
        "    gc.collect()\n",
        "    if device == \"cuda\":\n",
        "        torch.cuda.empty_cache()\n",
        "        print(f\"GPU memory freed: {torch.cuda.memory_allocated()/1e9:.2f} GB in use\")"
      ]
    }
  ]
}